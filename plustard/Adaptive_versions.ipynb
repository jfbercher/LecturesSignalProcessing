{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* &nbsp;\n",
    "\t* [Adaptive versions](#Adaptive-versions)\n",
    "\t\t* [The Least Mean Square (LMS) Algorithm ](#The-Least-Mean-Square-%28LMS%29-Algorithm)\n",
    "\t\t* [Illustation of the LMS in an identification problem](#Illustation-of-the-LMS-in-an-identification-problem)\n",
    "\t\t\t* [Identification procedure](#Identification-procedure)\n",
    "\t\t\t* [Stability of results](#Stability-of-results)\n",
    "\t\t\t* [Study with respect to $\\mu$](#Study-with-respect-to-$\\mu$)\n",
    "\t\t\t* [Tracking capabilities](#Tracking-capabilities)\n",
    "\t\t* [Convergence properties of the LMS\n",
    "](#Convergence-properties-of-the-LMS)\n",
    "\t\t* [The normalized LMS](#The-normalized-LMS)\n",
    "\t\t* [Other variants of the LMS](#Other-variants-of-the-LMS)\n",
    "\t\t* [Recursive Least Squares](#Recursive-Least-Squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Configuring matplotlib formats\n",
      "... Configuring matplotlib with inline figures\n",
      "... Importing numpy as np, scipy as sp, pyplot as plt, scipy.stats as stats\n",
      "   ... scipy.signal as sig\n",
      "... Importing widgets, display, HTML, Image, Javascript\n",
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\require{color}\n",
       "\\require{cancel}\n",
       "\\def\\tf#1{{\\mathrm{FT}\\left\\{ #1 \\right\\}}}\n",
       "\\def\\flecheTF{\\rightleftharpoons }\n",
       "\\def\\TFI#1#2#3{{\\displaystyle{\\int_{-\\infty}^{+\\infty} #1 ~e^{j2\\pi #2 #3} \n",
       "~\\dr{#2}}}}\n",
       "\\def\\TF#1#2#3{{\\displaystyle{\\int_{-\\infty}^{+\\infty} #1 ~e^{-j2\\pi #3 #2} \n",
       "~\\dr{#2}}}}\n",
       "\\def\\sha{ш}\n",
       "\\def\\dr#1{\\mathrm{d}#1}\n",
       "\\def\\egalpardef{\\mathop{=}\\limits^\\triangle}\n",
       "\\def\\sinc#1{{\\mathrm{sinc}\\left( #1 \\right)}}\n",
       "\\def\\rect{\\mathrm{rect}}\n",
       "\\definecolor{lightred}{rgb}{1,0.1,0}\n",
       "\\def\\myblueeqbox#1{{\\fcolorbox{blue}{lightblue}{$\textcolor{blue}{ #1}$}}}\n",
       "\\def\\myeqbox#1#2{{\\fcolorbox{#1}{light#1}{$\textcolor{#1}{ #2}$}}}\n",
       "\\def\\eqbox#1#2#3#4{{\\fcolorbox{#1}{#2}{$\\textcolor{#3}{ #4}$}}}\n",
       "% border|background|text\n",
       "\\def\\eqboxa#1{{\\boxed{#1}}}\n",
       "\\def\\eqboxb#1{{\\eqbox{green}{white}{green}{#1}}}\n",
       "\\def\\eqboxc#1{{\\eqbox{blue}{white}{blue}{#1}}}\n",
       "\\def\\eqboxd#1{{\\eqbox{blue}{lightblue}{blue}{#1}}}\n",
       "\\def\\E#1{\\mathbb{E}\\left[#1\\right]}\n",
       "\\def\\ta#1{\\left<#1\\right>}\n",
       "\\def\\egalparerg{{\\mathop{=}\\limits_\\mathrm{erg}}}\n",
       "\\def\\expo#1{\\exp\\left(#1\\right)}\n",
       "\\def\\d#1{\\mathrm{d}#1}\n",
       "\\def\\wb{\\mathbf{w}} \n",
       "\\def\\sb{\\mathbf{s}} \n",
       "\\def\\xb{\\mathbf{x}}\n",
       "\\def\\Rb{\\mathbf{R}} \n",
       "\\def\\rb{\\mathbf{r}} \n",
       "\\def\\mystar{{*}}\n",
       "\\def\\ub{\\mathbf{u}}\n",
       "\\def\\wbopt{\\mathop{\\mathbf{w}}\\limits^\\triangle}\n",
       "\\def\\deriv#1#2{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n",
       "\\def\\Ub{\\mathbf{U}}\n",
       "\\def\\db{\\mathbf{d}}\n",
       "\\def\\eb{\\mathbf{e}}\n",
       "\\def\\vb{\\mathbf{v}}\n",
       "\\def\\Ib{\\mathbf{I}}\n",
       "\\def\\Vb{\\mathbf{V}}\n",
       "\\def\\Lambdab{\\mathbf{\\Lambda}}\n",
       "\\def\\Ab{\\mathbf{A}}\n",
       "\\def\\Bb{\\mathbf{B}}\n",
       "\\def\\Cb{\\mathbf{C}}\n",
       "\\def\\Db{\\mathbf{D}}\n",
       "\\def\\Kb{\\mathbf{K}}\n",
       "\\def\\sinc#1{\\mathrm{sinc\\left(#1\\right)}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Defining figures captions \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".caption {\n",
       "font-weight: normal;\n",
       "text-align: left;\n",
       "width:60%; margin-left:10%; border:2px solid; padding-top:5px; padding-bottom:5px;\n",
       "background-color:white;border-color:#efd3d7;color:black;\n",
       "border-radius:8px;-webkit-border-radius:8px;-moz-border-radius:8px;border-radius:8px\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loading customized Javascript for interactive solutions (show/hide)\n"
     ]
    }
   ],
   "source": [
    "%run nbinit.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "import warnings\n",
    "warnings.simplefilter('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steepest descent algorithm employs the gradient of the mean square error to search for the Wiener filter coefficients. The drawbacks are that \n",
    "- this relies on the knowledge of the true second-order statistics (correlations), while they are evidently non available;\n",
    "- the resulting filter is not adaptive to a non-stationary environment, since the normal equations have been derived in a stationary context.\n",
    "\n",
    "In order to take into account those two drawbacks, we need to define `estimates of the correlation functions` able track non-stationarities of signals. With these estimates at hand, we will just have to plug them in the normal equations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the simple example where we have to estimate the power of a non-stationary signal:\n",
    "$$\n",
    "\\sigma(n)^2 = \\E{X(n)^2}.\n",
    "$$\n",
    "A simple solution is to approximate the ensemble average as a time average is some neighborhood of point $n$:\n",
    "$$\n",
    "\\sigma_L(n)^2 = \\frac{1}{2L+1}\\sum_{l=-L}^{L} x(n-l)^2.\n",
    "$$\n",
    "which corresponds to filtering with a sliding (rectangular) window of length $2L+1$. Note that it is possible to compute this recursively as\n",
    "$$\n",
    "\\sigma_L(n)^2 = \\sigma_L(n-1)^2  + x(n+L)-x(n-L-1).\n",
    "$$\n",
    "\n",
    "Another solution is to introduce a forgetting factor $\\lambda$ which enables to give more weight to the more recent samples and forget the older ones. The corresponding formula is\n",
    "$$\n",
    "\\sigma_\\lambda(n)^2 = K_n\\sum_{l=0}^n \\lambda^{n-l} x(l)^2,\n",
    "$$\n",
    "where $K_n$ is a factor which ensures unbiaseness of the estimate, i.e. $\\E{\\sigma_\\lambda(n)^2}=\\sigma(n)^2$. As an exercise, you should check that $K_n = (1-\\lambda^{n+1})/(1-\\lambda)$. For $\\lambda<1$, $K_n$ converges rapidly and we may take it as a constant. In such case, denoting \n",
    "$$\n",
    "s_\\lambda(n)^2=  \\sigma_\\lambda(n)^2/K, \n",
    "$$\n",
    "we have a simple recursive formula:\n",
    "$$\n",
    "s_\\lambda(n)^2 = \\lambda s_\\lambda(n-1)^2 + x(n)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines simulate a non-stationary signal with time-varying power. We implement the exponential average for estimating the power. You should experiment with the values of $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=1000\n",
    "from scipy.special import expit  # logistic function\n",
    "x=np.random.normal(size=N)\n",
    "t=np.linspace(-6,6,N)\n",
    "z=x*(2*expit(t)-1)\n",
    "\n",
    "def plt_vs_lambda(lamb):\n",
    "    plt.plot(t,z,alpha=0.4,label='Observations')\n",
    "    #We implement $s_\\lambda(n)^2 = \\lambda s_\\lambda(n-1)^2 + x(n)^2.$\n",
    "    slambda=np.zeros(N)\n",
    "    for n in np.arange(1,N):\n",
    "        slambda[n]=lamb*slambda[n-1]+z[n]**2\n",
    "    plt.plot(t,slambda*(1-lamb),lw=3,alpha=0.6,label='Estimate of the instantaneous power')\n",
    "    plt.plot(t,(2*expit(t)-1)**2,lw=2,label='Instantaneous power')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "lamb=widgets.FloatSlider(min=0,max=1,value=0.8, step=0.01)\n",
    "_=interact(plt_vs_lambda, lamb=lamb)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us return to the normal equation (\\ref{eq:wopt}):\n",
    "$$\n",
    "\\wbopt=\\Rb_{uu}^{-1} \\Rb_{du}\n",
    "$$\n",
    "and to  its SDA version (\\ref{eq:grad}):\n",
    "\\begin{align}\n",
    "\\wb(n+1) & = \\wb(n) - \\mu \\E{ \\ub(n) e(n) } \\\\\n",
    "& = \\wb(n) - \\mu \\left(\\Rb_{uu} \\wb(n) -  \\Rb_{du} \\right) \\\\\n",
    "\\end{align}\n",
    "We will substitute the true values with estimated ones. \n",
    "An important remark is that the result of the normal equation is insensitive to a scale factor on the estimates. It is thus possible to estimate the correlation matrix and vector using a sliding average\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat\\Rb_{uu}(n) = \\sum_{l=-L}^L \\ub(n-l)\\ub(n-l)^H \\\\\n",
    "\\hat\\Rb_{du}(n) = \\sum_{l=-L}^L \\d(n-l)\\ub(n-l) \n",
    "\\end{cases}\n",
    "$$\n",
    "or by an exponential mean\n",
    "$$\n",
    "\\hat\\Rb_{uu}(n) = \\sum_{l=0}^n \\lambda^{l-n} \\ub(l)\\ub(l)^H = \\lambda \\hat\\Rb_{uu}(n-1) + \\ub(n)\\ub(n)^H\n",
    "$$\n",
    "which yields\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat\\Rb_{uu}(n) = \\sum_{l=0}^n \\lambda^{l-n} \\ub(l)\\ub(l)^H = \\lambda \\hat\\Rb_{uu}(n-1) + \\ub(n)\\ub(n)^H\\\\\n",
    "\\hat\\Rb_{du}(n) = \\lambda \\hat\\Rb_{du}(n-1) + d(n) \\ub(n).\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Least Mean Square (LMS) Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest estimator that can be defined is the limit case where we do not average at all... That is we take either $L=0$ or $\\lambda=0$ in the previous formulas, to get the `instantaneous estimates`  \n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat\\Rb_{uu}(n) =  \\ub(n)\\ub(n)^H\\\\\n",
    "\\hat\\Rb_{du}(n) = d(n) \\ub(n).\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "This merely consists in suppressing the expectations in the theoretical formulas. So doing, we obtain formulas which directly depend on the data, with no need to know something on the theoretical statistics, and which also depend on time, thus confering adaptivity to the algorithm. Plugging these estimates in the SDA, we obtain\n",
    "\\begin{align}\n",
    "\\wb(n+1) & = \\wb(n) - \\mu  \\ub(n) e(n)  \\\\\n",
    "& = \\wb(n) - \\mu \\ub(n) \\left(\\ub(n)^H \\wb(n) -  d(n) \\right) \\\\\n",
    "\\label{eq:lms}\n",
    "\\end{align}\n",
    "\n",
    "Substituting $\\ub(n)\\ub(n)^H$ for $\\E{\\ub(n)\\ub(n)^H}$, or ${ \\ub(n) e(n) }$ for $\\E{ \\ub(n) e(n) }$ is really a  crude approximation. Nevertheless, the  averaging occurs by the iterating process so that this kind of method works. The LMS algorithm is  by far the most commonly used adaptive filtering algorithm, because it is extremely simple to implement, has a very low computational load, works relatively well and has tracking capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to illustrate the behavior of the LMS algorithm, we continue the example of the identification of an unknown system. We first recreate the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustation of the LMS in an identification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "# test\n",
    "figplot=False\n",
    "N=800\n",
    "x=lfilter([1, 1], [1], np.random.randn(N))\n",
    "htest=10*np.array([1, 0.7, 0.7, 0.7, 0.3, 0 ])\n",
    "y0=lfilter(htest,[1],x)\n",
    "y=y0+0.1*randn(N)\n",
    "if figplot:\n",
    "    plt.plot(y)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.title(\"Observation\")\n",
    "    figcaption(\"System output in an identification problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since one should do it at least one time, try to implement a LMS algorithm. You will will define a function with the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lms(d,u,w,mu):\n",
    "    \"\"\" \n",
    "    Implements a single iteration of the stochastic gradient (LMS)\\n\n",
    "    :math:`w(n+1)=w(n)+\\\\mu u(n)\\\\left(d(n)-w(n)^T u(n)\\\\right)̀`\n",
    "    \n",
    "    Input:\n",
    "    ======\n",
    "        d : desired sequence at time n \n",
    "        u : input of length p\n",
    "        w : wiener filter to update \n",
    "        mu : adaptation step\n",
    "    \n",
    "    Returns:\n",
    "    =======\n",
    "        w : upated filter\n",
    "        err : d-dest\n",
    "        dest : prediction = :math:`u(n)^T w` \n",
    "    \"\"\"\n",
    "#\n",
    "# DO IT YOURSELF!\n",
    "#\n",
    "    return (w,err,dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may test your function using the following validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "wout, errout, destout = lms(np.random.normal(1),np.random.normal(6),np.zeros(6),0.05)\n",
    "wtest = np.array([ 0.76063565,  0.76063565,  0.76063565,  0.76063565,  0.76063565, 0.76063565])\n",
    "#Test\n",
    "if np.shape(wout)==np.shape(wtest):\n",
    "    if np.sum(np.abs(wout-wtest))<1e-8:\n",
    "        print(\"Test validated\")\n",
    "    else:\n",
    "        print(\"There was an error in implementation\")    \n",
    "else:\n",
    "    print(\"Error in dimensions\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible implementation is given now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "manualexec": true,
    "widget": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "        console.log(typeof pr_cell_sols)\n",
       "        if (typeof pr_cell_sols !== \"undefined\") {\n",
       "        pr_cell_sols(1)\n",
       "        } else console.log(\"on reload, process_cell undefined\")\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_solution(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cbox_ck": true,
    "cbox_id": "myCheck2",
    "collapsed": false,
    "hidden": false,
    "widget": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"onoffswitch\">\n",
       "<input type=\"checkbox\" name=\"onoffswitch\" class=\"onoffswitch-checkbox\"\n",
       "onclick=\"document.getElementById('myCheck2').checked ? \n",
       "show_input(1) : hide_input(1)\" \n",
       "id=\"myCheck2\"  checked>\n",
       "<label class=\"onoffswitch-label\" for=\"myCheck2\"> \n",
       "<span class=\"onoffswitch-inner\"></span> \n",
       "<span class=\"onoffswitch-switch\"></span>\n",
       "</label>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display, Javascript\n",
    "display(HTML('<div class=\"onoffswitch\">\\n<input type=\"checkbox\" name=\"onoffswitch\" class=\"onoffswitch-checkbox\"\\nonclick=\"document.getElementById('+\"'myCheck2'\"+').checked ? \\nshow_input(1) : hide_input(1)\" \\nid=\"myCheck2\"  checked>\\n<label class=\"onoffswitch-label\" for=\"myCheck2\"> \\n<span class=\"onoffswitch-inner\"></span> \\n<span class=\"onoffswitch-switch\"></span>\\n</label>\\n</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Begin by some direct commands (initializations and a `for` loop on the time variable) for identifying the filter; once this works you will implement th commands as a function `ident` \n",
    "- If necessary, the function `squeeze()` enable to remove single-dimensional entries from the shape of an n-D array  (*e.g.* transforms an array (3,1,1) into a vector of dimension 3)\n",
    "\n",
    "In order to evaluate the algorithm behavior, you will plot the estimation error, the evolution of the coefficients of the identified filter during the iterations of the algorithm; and finally the quadratic error between the true filter and the identified one. This should be done for several orders $p$ (the exact order is unknown...) and for different values of the adaptation step $\\mu$. \n",
    "\n",
    "- The quadratic error can be evaluated simply thanks to a  *comprehension list* according to \n",
    "         Errh=[sum(he-w[:,n])**2 for n in range(N+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the code below, and implement the missing lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu=0.1 # an initial value for mu\n",
    "L=6  # size of identified filter (true size is p)\n",
    "NN=200 #number of iterations\n",
    "err=np.zeros(NN)\n",
    "w=zeros((L,NN+1))\n",
    "yest=np.zeros(NN)\n",
    "\n",
    "# The key lines are here: you have to iterate over time and compute\n",
    "# the output of the LMS at each iteration.You may save all outputs in the matrix \n",
    "# w initialized above -- column k contains the solution at time k. You must \n",
    "# also save the succession of errors, and the estimated output\n",
    "#\n",
    "# You have two lines to implement here.\n",
    "# DO IT YOURSELF!\n",
    "#\n",
    "# After these lines, (w[:,t+1],err[t],yest[t]) are defined\n",
    "\n",
    "\n",
    "# This is used to define the \"true\" impulse response  vector with the same size as w:\n",
    "# a shorter (truncated) one if L<p, and a larger one (zero-padded) if L>p.\n",
    "newhtest=np.zeros(L)\n",
    "if np.size(htest)<L:\n",
    "    newhtest=htest[:L]\n",
    "else:\n",
    "    newhtest[:np.size(htest)]=htest\n",
    "    \n",
    "# Results:\n",
    "plt.figure(1)\n",
    "tt=np.arange(NN)\n",
    "plt.plot(tt,y0[:NN],label='Initial Noiseless Output')\n",
    "plt.plot(tt,yest[:NN], label=\"Estimated Output\")\n",
    "plt.xlabel('Time')\n",
    "figcaption(\"Comparison of true output and estimated one after identification\", \n",
    "           label=\"fig:ident_lms_compareoutputs\")\n",
    "\n",
    "plt.figure(2)\n",
    "errh=[sum((newhtest-w[:,t])**2) for t in range(NN)]\n",
    "plt.plot(tt,errh,label='Quadratic error on h')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "figcaption(\"Quadratic error between true and estimated filter\", \n",
    "           label=\"fig:ident_lms_eqonh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "manualexec": true,
    "widget": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_solution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b18628453ddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_solution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'show_solution' is not defined"
     ]
    }
   ],
   "source": [
    "show_solution(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mu=0.05 # an initial value for mu\n",
    "L=6  # size of identified filter (true size is p)\n",
    "NN=200 #number of iterations\n",
    "err=np.zeros(NN)\n",
    "w=zeros((L,NN+1))\n",
    "yest=np.zeros(NN)\n",
    "\n",
    "\n",
    "# The key lines are here: you have to iterate over time and compute\n",
    "# the output of the LMS at each iteration.You may save all outputs in the matrix \n",
    "# w initialized above -- column k contains the solution at time k. You must \n",
    "# also save the succession of errors, and the estimated output\n",
    "\n",
    "for t in np.arange(L,NN):\n",
    "    (w[:,t+1],err[t],yest[t])=lms(y[t],x[t:t-L:-1],w[:,t],mu)\n",
    "\n",
    "\n",
    "# This is used to define the \"true\" impulse response  vector with the same size as w:\n",
    "# a shorter (truncated) one if L<p, and a larger one (zero-padded) if L>p.\n",
    "newhtest=np.zeros(L)\n",
    "LL=np.min([np.size(htest),L])\n",
    "newhtest[:LL]=htest[:LL]\n",
    "      \n",
    "# Results:\n",
    "plt.figure(1)\n",
    "tt=np.arange(NN)\n",
    "plt.plot(tt,y0[:NN],label='Initial Noiseless Output')\n",
    "plt.plot(tt,yest[:NN], label=\"Estimated Output\")\n",
    "plt.xlabel('Time')\n",
    "figcaption(\"Comparison of true output and estimated one after identification\", \n",
    "           label=\"fig:ident_lms_compareoutputs\")\n",
    "\n",
    "plt.figure(2)\n",
    "errh=[sum((newhtest-w[:,t])**2) for t in range(NN)]\n",
    "plt.plot(tt,errh,label='Quadratic error on h')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "figcaption(\"Quadratic error between true and estimated filter\", \n",
    "           label=\"fig:ident_lms_eqonh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the identification as a function on its own, which simply maked some initializations and use a loop on the LMS. Implement this function according to the following syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ident(observation,input_data,mu,p=20,h_initial=zeros(20)):\n",
    "    \"\"\" Identification of an impulse response from an observation\n",
    "    `observation` of its output, and from its input `input_data` \n",
    "    `mu` is the adaptation step\\n\n",
    "    Inputs:\n",
    "    =======\n",
    "    observation: array\n",
    "        output of the filter to identify\n",
    "    input_data: array\n",
    "        input of the filter to identify\n",
    "    mu: real\n",
    "        adaptation step\n",
    "    p: int (default =20)\n",
    "        order of the filter\n",
    "    h_initial: array (default h_initial=zeros(20))\n",
    "        initial guess for the filter\n",
    "    normalized: boolean (default False)    \n",
    "        compute the normalized LMS instead of the standard one\n",
    "    \n",
    "    Outputs:\n",
    "    ========\n",
    "    w: array\n",
    "        identified impulse response\n",
    "    err: array\n",
    "        estimation error\n",
    "    yest: array\n",
    "        estimated output\n",
    "    \"\"\"\n",
    "    N=np.size(input_data)\n",
    "    err=np.zeros(N)\n",
    "    w=np.zeros((p,N+1))\n",
    "    yest=np.zeros(N)\n",
    "\n",
    "   #\n",
    "   # DO IT YOURSELF!\n",
    "   #\n",
    "        \n",
    "    return (w,err,yest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cbox_ck": true,
    "cbox_id": "myCheck1",
    "collapsed": false,
    "manualexec": true,
    "widget": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"onoffswitch\">\n",
       "<input type=\"checkbox\" name=\"onoffswitch\" class=\"onoffswitch-checkbox\"\n",
       "onclick=\"document.getElementById('myCheck1').checked ? \n",
       "show_input(1) : hide_input(1)\" \n",
       "id=\"myCheck1\"  checked>\n",
       "<label class=\"onoffswitch-label\" for=\"myCheck1\"> \n",
       "<span class=\"onoffswitch-inner\"></span> \n",
       "<span class=\"onoffswitch-switch\"></span>\n",
       "</label>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display, Javascript\n",
    "display(HTML('<div class=\"onoffswitch\">\\n<input type=\"checkbox\" name=\"onoffswitch\" class=\"onoffswitch-checkbox\"\\nonclick=\"document.getElementById('+\"'myCheck1'\"+').checked ? \\nshow_input(1) : hide_input(1)\" \\nid=\"myCheck1\"  checked>\\n<label class=\"onoffswitch-label\" for=\"myCheck1\"> \\n<span class=\"onoffswitch-inner\"></span> \\n<span class=\"onoffswitch-switch\"></span>\\n</label>\\n</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": false
   },
   "outputs": [],
   "source": [
    "def ident(observation,input_data,mu,p=20,h_initial=zeros(20),normalized=False):\n",
    "    \"\"\" Identification of an impulse response from an observation\n",
    "    `observation` of its output, and from its input `input_data` \\n\n",
    "    `mu` is the adaptation step\\n\n",
    "    Inputs:\n",
    "    =======\n",
    "    observation: array\n",
    "        output of the filter to identify\n",
    "    input_data: array\n",
    "        input of the filter to identify\n",
    "    mu: real\n",
    "        adaptation step\n",
    "    p: int (default =20)\n",
    "        order of the filter\n",
    "    h_initial: array (default h_initial=zeros(20))\n",
    "        initial guess for the filter\n",
    "    Outputs:\n",
    "    ========\n",
    "    w: array\n",
    "        identified impulse response\n",
    "    err: array\n",
    "        estimation error\n",
    "    yest: array\n",
    "        estimated output\n",
    "    \"\"\"\n",
    "    N=np.size(input_data)\n",
    "    input_data=squeeze(input_data) #reshape(input_data,(N))\n",
    "    observation=squeeze(observation)\n",
    "    err=np.zeros(N)\n",
    "    w=np.zeros((p,N+1))\n",
    "    yest=np.zeros(N)\n",
    "\n",
    "    w[:,p]=h_initial\n",
    "    for t in range(p,N):   \n",
    "        if normalized:\n",
    "            mun=mu/(dot(input_data[t:t-p:-1],input_data[t:t-p:-1])+1e-10)   \n",
    "        else:\n",
    "            mun=mu\n",
    "        (w[:,t+1],err[t],yest[t])=lms(observation[t],input_data[t:t-p:-1],w[:,t],mun)\n",
    "        \n",
    "    return (w,err,yest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation can simply be tested with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L=8\n",
    "(w,err,yest)=ident(y,x,mu=0.05,p=L,h_initial=zeros(L))\n",
    "\n",
    "newhtest=np.zeros(L)\n",
    "LL=np.min([np.size(htest),L])\n",
    "newhtest[:LL]=htest[:LL]\n",
    "\n",
    "NN=np.min([np.size(yest),200])\n",
    "errh=[sum((newhtest-w[:,t])**2) for t in range(NN)]\n",
    "plt.plot(tt,errh,label='Quadratic error on h')\n",
    "plt.legend()\n",
    "_=plt.xlabel('Time')\n",
    "print(\"Identified filter: \",w[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ident(observation,input_data,mu,p=20,h_initial=zeros(20),normalized=False):\n",
    "    \"\"\" Identification of an impulse response from an observation\n",
    "    `observation` of its output, and from its input `input_data` \n",
    "    `mu` is the adaptation step\\n\n",
    "    Inputs:\n",
    "    =======\n",
    "    observation: array\n",
    "        output of the filter to identify\n",
    "    input_data: array\n",
    "        input of the filter to identify\n",
    "    mu: real\n",
    "        adaptation step\n",
    "    p: int (default =20)\n",
    "        order of the filter\n",
    "    h_initial: array (default h_initial=zeros(20))\n",
    "        initial guess for the filter\n",
    "    normalized: boolean (default False)    \n",
    "        compute the normalized LMS instead of the standard one\n",
    "    \n",
    "    Outputs:\n",
    "    ========\n",
    "    w: array\n",
    "        identified impulse response\n",
    "    err: array\n",
    "        estimation error\n",
    "    yest: array\n",
    "        estimated output\n",
    "    \"\"\"\n",
    "    N=np.size(input_data)\n",
    "    err=np.zeros(N)\n",
    "    w=np.zeros((p,N+1))\n",
    "    yest=np.zeros(N)\n",
    "\n",
    "    w[:,p]=h_initial\n",
    "    for t in np.arange(p,N):   \n",
    "        if normalized:\n",
    "            assert mu<2, \"In the normalized case, mu must be less than 2\"\n",
    "            mun=mu/(np.dot(input_data[t:t-p:-1],input_data[t:t-p:-1])+1e-10)   \n",
    "        else:\n",
    "            mun=mu\n",
    "        (w[:,t+1],err[t],yest[t])=lms(observation[t],input_data[t:t-p:-1],w[:,t],mun)\n",
    "        \n",
    "    return (w,err,yest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very instructive to look at the reproductibility of results when the data change. Let $\\mu$ fixed and generate new data. Then apply  the identification procedure and plot the learning curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=6 #<-- actual length of the filter\n",
    "for ndata in range(30):\n",
    "    ## Generate new datas\n",
    "    N=200\n",
    "    x=lfilter([1, 1], [1], np.random.randn(N))\n",
    "    htest=10*np.array([1, 0.7, 0.7, 0.7, 0.3, 0 ])\n",
    "    y0=lfilter(htest,[1],x)\n",
    "    y=y0+0.1*randn(N)\n",
    "    iterations=np.arange(NN+1)\n",
    "    # --------------------\n",
    "\n",
    "    for mu in [0.01]:\n",
    "        (w,erreur,yest)=ident(y,x,mu,p=p,h_initial=zeros(p))\n",
    "        Errh=[sum(htest-w[:,n])**2 for n in range(NN+1)]\n",
    "        plt.plot(iterations,Errh, label=\"$\\mu={}$\".format(mu))\n",
    "        plt.xlim([0, NN+1])\n",
    "\n",
    "\n",
    "plt.title(\"Norm of the error to the optimum filter\")\n",
    "_=plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are random; the algorithm is stochastic and so is the learning curve! Fortunately, we still check that the algorithms converge... since the error goes to zero. So, it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study with respect to $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really a simple task to study the behavior with respect to the choice of the stepsize $\\mu$. We just have to make a loop over possible values of $\\mu$, call the identification procedure and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # Study with respect to $\\mu$    \n",
    "p=6\n",
    "NN=100\n",
    "iter=np.arange(NN+1)-p\n",
    "\n",
    "## Generate new datas\n",
    "N=200\n",
    "x=lfilter([1, 1], [1], np.random.randn(N))\n",
    "htest=10*np.array([1, 0.7, 0.7, 0.7, 0.3, 0 ])\n",
    "y0=lfilter(htest,[1],x)\n",
    "y=y0+0.1*randn(N)\n",
    "# --------------------\n",
    "\n",
    "for mu in [0.01, 0.02, 0.05, 0.081]:\n",
    "    (w,erreur,yest)=ident(y,x,mu,p=p,h_initial=zeros(p))\n",
    "    Errh=[sum(htest-w[:,n])**2 for n in range(NN+1)]\n",
    "    plt.plot(iter,Errh, label=\"$\\mu={}$\".format(mu))\n",
    "    plt.xlim([0, NN+1])\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Norm of the error to the optimum filter\")\n",
    "_=plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a constant step-size, the LMS never converge, since while an error exist, the filter is always updated. A consequence of this fact is that the LMS keeps tracking capabilities, which are especially useful in a non-stationary context. In the identification concept, it is possible that the filter to be identified varies during time. In such case, the algorithm must be able to track these modifications. Such an example is simulated below, where the impulse response is modulated by a $\\cos()$, according to\n",
    "$$\n",
    "h(t,\\tau) = \\left(1+ \\cos(2\\pi f_0 t)\\right) h_\\mathrm{test}(\\tau).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Slow non-stationarity\n",
    "\n",
    "N=1000\n",
    "u=np.random.randn(N)\n",
    "y=np.zeros(N)\n",
    "htest=10*np.array([1, 0.7, 0.7, 0.7, 0.3, 0 ])\n",
    "L=size(htest)\n",
    "for t in np.arange(L,N):\n",
    "    y[t]=dot((1+cos(2*pi*t/N))*htest,u[t:t-L:-1])\n",
    "y+=0.01*np.random.randn(N)\n",
    "plt.figure()\n",
    "plt.plot(y)\n",
    "_=plt.title(\"Observed Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can test the identification procedure for this non stationary signal. We check that the error indeed goes to zero, and that the identified filter seem effectively modulated with a cosine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=7\n",
    "(w,err,yest)=ident(y,u,mu=0.1,p=p,h_initial=zeros(p))\n",
    "#(w,err,yest)=ident(y,u,mu=1,p=p,h_initial=zeros(p),normalized=True)\n",
    "plt.figure(1)\n",
    "clf()\n",
    "plt.plot(err)\n",
    "plt.title('Identification error')\n",
    "figcaption(\"Identification error in the nonstationary case\", label=\"fig:error_ns_case\")\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "t=np.arange(0,N+1)\n",
    "true_ns_h=np.outer((1+cos(2*pi*t/N)),htest)\n",
    "plt.plot(t,w.T,lw=1)\n",
    "plt.plot(t,true_ns_h,lw=2,label=\"True values\", alpha=0.4)\n",
    "plt.title(\"Evolution of filter's coefficients\")\n",
    "figcaption(\"Evolution of filter's coefficients\", label=\"fig:coeff_ns_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence properties of the LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we realize with these numerical experiments, since the LMS directly depends of the data, the algorithm itself is stochastic; the learning curves have a random character but the mean trajectories still converge to the correct solution. The correct characterization of stochastic algorithms is difficult -- actually, the first correct analysis is due to Eweda and Macchi (1983). The traditional analysis relies on a false hypothesis, the *independence assumption*, which still gives a good idea of what happens.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is simply that the average algorithm \n",
    "\\begin{align}\n",
    "\\E{\\wb(n+1)} & = \\E{\\wb(n) - \\mu {\\ub(n)\\left(\\ub(n)^T\\wb(n)-d(n) \\right) }} \\\\\n",
    "& =   \\E{\\wb(n)} - \\mu \\E{\\ub(n)\\left(\\ub(n)^T\\wb(n)-d(n) \\right) } \\\\\n",
    "& \\approx   \\E{\\wb(n)} - \\mu \\left( \\E{\\ub(n)\\ub(n)^T}\\E{\\wb(n)}-\\E{\\ub(n)d(n)} \\right)\n",
    "\\end{align}\n",
    "is exactly the true gradient algorithm. Thus, we would have exactly the same conditions for convergence as  for the gradient algorithm. However, this is only an approximation. Indeed, in the third line the equality $\\E{\\ub(n)\\ub(n)^T\\wb(n)}=\\E{\\ub(n)\\ub(n)^T}\\E{\\wb(n)}$ is incorrect since obviously $\\wb(n)$ depends on $\\ub(n)$ through the components at times $n-1, n-2,$ etc. \n",
    "\n",
    "Furthermore, it must be stressed that the learning curves are now **random**. Thus, we can understand that the convergence conditions are more strict than for the gradient algorithm. A practical rule for the choice of $\\mu$ is \n",
    "$$\n",
    "\\mu = \\frac{2}{\\alpha \\mathrm{Tr}\\left[\\Rb_{uu}\\right]} = \\frac{2}{\\alpha pR_{uu}(0)},\n",
    "$$\n",
    "where $\\alpha$ is a scalar between 2 and 3, $R_{uu}(0)=\\E{|u(n)|^2}$ and $p$ is the dimension of the correlation matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... to be continued..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Eweda, E., and Macchi, O.. \"Quadratic mean and almost-sure convergence of unbounded stochastic approximation algorithms with correlated observations.\" Annales de l'institut Henri Poincaré (B) Probabilités et Statistiques 19.3 (1983): 235-255. <http://eudml.org/doc/77211>.\n",
    "\n",
    "@article{Eweda1983,\n",
    "author = {Eweda, E., Macchi, O.},\n",
    "journal = {Annales de l'institut Henri Poincaré (B) Probabilités et Statistiques},\n",
    "keywords = {almost-sure convergence; correlated observations; quadratic mean convergence; stochastic gradient algorithm; finite memory; finite moments},\n",
    "language = {eng},\n",
    "number = {3},\n",
    "pages = {235-255},\n",
    "publisher = {Gauthier-Villars},\n",
    "title = {Quadratic mean and almost-sure convergence of unbounded stochastic approximation algorithms with correlated observations},\n",
    "url = {http://eudml.org/doc/77211},\n",
    "volume = {19},\n",
    "year = {1983},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The normalized LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple variant of the LMS relies on the idea of introducing an non constant step-size $\\mu_n$ and to determine an optimum value for the step-size at each iteration. A simple way to show the result is as follows. \n",
    "- The standard error, before updating the LMS from $\\wb(n)$ into $\\wb(n+1)$,  is \n",
    "$$e(n|n)=\\wb(n)^T\\ub(n) - d(n)$$\n",
    "- After having updated the filter, we can recompute the error, as\n",
    "$$e(n|n+1)=\\wb(n+1)^T\\ub(n) - d(n).$$\n",
    "This error is called *a posteriori* error, since it is calculated with the updated filter. This is also indicated by the notation $.|n+1$ which means \"computed using the filter at time $n+1$.  The standard error is thus qualified of *a priori* error. \n",
    "\n",
    "Since $\\wb(n+1)=\\wb(n)-\\mu\\ub(n)e(n|n)$, we immediately get that\n",
    "\\begin{align}\n",
    "e(n|n+1)& =\\wb(n+1)^T\\ub(n) - d(n) \\\\\n",
    "& = \\left(\\wb(n)-\\mu_n\\ub(n)e(n|n)\\right)^T\\ub(n) - d(n) \\\\\n",
    "& = e(n|n)-\\mu_n\\ub(n)^T\\ub(n)e(n|n) \\\\\n",
    "& = \\left(1-\\mu_n\\ub(n)^T\\ub(n)\\right) e(n|n) \n",
    "\\end{align}\n",
    "Evidently, updating must decrease the error. Thus, we must have \n",
    "$$\n",
    "|e(n|n+1)| \\leq |e(n|n)|\n",
    "$$\n",
    "that is\n",
    "$$\n",
    "\\left|\\left(1-\\mu_n\\ub(n)^T\\ub(n)\\right)\\right| \\leq 1.\n",
    "$$\n",
    "This yields the condition\n",
    "$$\n",
    "\\eqboxc{\n",
    "0\\leq\\mu_n \\leq \\frac{2}{\\ub(n)^T\\ub(n)}\n",
    "}.\n",
    "\\label{eq:murangenlms}\n",
    "$$\n",
    "The optimum value of the step-size corresponds to the minimum of |e(n|n+1)|, which is simply given by \n",
    "$$\n",
    "\\eqboxc{\n",
    "\\mu_n = \\frac{1}{\\ub(n)^T\\ub(n)}\n",
    "}.\n",
    "$$\n",
    "However, the **normalized LMS algorithm** is often given with an auxiliary factor, say $\\tilde\\mu$, which adds a tuning parameter the algorithm\n",
    "$$\n",
    "\\eqboxd{\n",
    "\\wb(n+1)=\\wb(n)-\\frac{\\tilde\\mu}{\\ub(n)^T\\ub(n)} \\ub(n)\\left(\\wb(n)^T\\ub(n) -d(n)\\right) \n",
    "}\n",
    "$$\n",
    "The condition (\\ref{eq:murangenlms}) directly gives\n",
    "$$\n",
    "\\eqboxd{0\\leq\\tilde\\mu\\leq2},\n",
    "\\label{eq:murangenlms_2}\n",
    "$$\n",
    "which is a very simple rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the normalized LMS is a simple modification of the standard LMS. Note that it is useful to introduce a small positive constant in the definition of the step-size\n",
    "$$\n",
    "\\mu_n = \\frac{1}{\\ub(n)^T\\ub(n)+\\epsilon}\n",
    "$$\n",
    "in order to avoid division by zero errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalized_lms(d,u,w,mu):\n",
    "    \"\"\" \n",
    "    Implements a single iteration of the stochastic gradient (LMS)\\n\n",
    "    :math:`w(n+1)=w(n)+\\\\mu u(n)\\\\left(d(n)-w(n)^T u(n)\\\\right)̀`\n",
    "    \n",
    "    Input:\n",
    "    ======\n",
    "        d : desired sequence at time n \n",
    "        u : input of length p\n",
    "        w : wiener filter to update \n",
    "        mu : adaptation step for the NLMS; mu <2\n",
    "    \n",
    "    Returns:\n",
    "    =======\n",
    "        w : upated filter\n",
    "        err : d-dest\n",
    "        dest : prediction = :math:`u(n)^T w` \n",
    "    \"\"\"\n",
    "    assert mu<2, \"In the normalized case, mu must be less than 2\"\n",
    "    u=squeeze(u) #Remove single-dimensional entries from the shape of an array.\n",
    "    w=squeeze(w)\n",
    "    dest=u.dot(w)\n",
    "    err=d-dest\n",
    "    mun=mu/(dot(u,u)+1e-10)  \n",
    "    w=w+mun*u*err\n",
    "    return (w,err,dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other variants of the LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient algorithm is obtained from the theoretical gradient algorithm by approximating the exact statistical quantities by their instantaneous values. This approach can be extended to arbitrary cost functions. Indeed, if we consider a cost function $J(\\wb)=\\E{f(e(n))}$, with $f$ a positive even function, then the steepest descent algorithm leads to\n",
    "\\begin{align}\n",
    "\\wb(n+1) & =\\wb(n) - \\mu \\deriv{\\E{f(e(n))}}{\\wb(n)} \\\\\n",
    "& = \\wb(n) - \\mu \\E{\\ub(n) \\deriv{f(e(n))}{\\wb(n)}},\\\\\n",
    "\\end{align}\n",
    "where we used the chain rule for derivation.\n",
    "\n",
    "The corresponding stochastic gradient algorithm is then immediately given by\n",
    "$$\n",
    "\\wb(n+1) = \\wb(n) - \\mu {\\ub(n) \\deriv{f(e(n))}{e(n)}}.\n",
    "$$\n",
    "Let us look at some examples:\n",
    "\n",
    "- if $f(e)=|e|$, then $f'(e)=\\mathrm{sign}(e)$ and we obtain the so-called **sign-error** algorithm:\n",
    "$$\n",
    "\\wb(n+1) = \\wb(n) - \\mu {\\ub(n)\\mathrm{sign}(e(n)) }.\n",
    "$$\n",
    "This is an early algorithm with very low complexity, which can be implemented without any multiplications (if $\\mu$ is a power of 2, then the step-size multiplication can be \n",
    "implemented as a bit shift).\n",
    "- for $f(e)=|e|^k$, then  $f'(e)=k|e|^{k-1}\\mathrm{sign}(e)$, and the stochastic gradient algorithm has the form\n",
    "$$\n",
    "\\wb(n+1) = \\wb(n) - \\mu {\\ub(n)|e(n)|^{k-1}\\mathrm{sign}(e(n)) }.\n",
    "$$\n",
    "\n",
    "See [http://www.ece.utah.edu/~mathews/ece6550/chapter4.pdf](http://www.ece.utah.edu/~mathews/ece6550/chapter4.pdf), page 22, for an example of a piecewise linear cost function leading to a quantization of the error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of taking an instantaneous estimate of the correlation matrix and vector, it is still possible to go on with the exponential mean estimates\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat\\Rb_{uu}(n+1) = \\sum_{l=0}^{n+1} \\lambda^{l-n-1} \\ub(l)\\ub(l)^H = \\lambda \\hat\\Rb_{uu}(n) + \\ub(n+1)\\ub(n+1)^H\\\\\n",
    "\\hat\\Rb_{du}(n+1) = \\lambda \\hat\\Rb_{du}(n) + d(n+1) \\ub(n+1).\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "It remains to compute the solution\n",
    "\\begin{equation}\n",
    "\\hat\\wb(n+1) =  \\left[\\hat\\Rb_{uu}(n+1)\\right]^{-1} \\hat\\Rb_{du}(n+1).\n",
    "\\end{equation}\n",
    "The main problem is the inversion, for each $n$, of the correlation matrix. Fortunately, it is possible to obtain a recursive  solution which do not need a matrix inversion at all...\n",
    "The key here is to invoke the [matrix inversion lemma](http://en.wikipedia.org/wiki/Woodbury_matrix_identity)\n",
    "\\begin{equation}\n",
    "[\\Ab + \\Bb\\Db]^{-1} = \\Ab^{-1} - \\Ab^{-1}\\Bb[\\Ib + \n",
    "\\Db\\Ab^{-1}\\Bb]^{-1}\\Db\\Ab^{-1}.\n",
    "\\end{equation}\n",
    "Applying this with $\\Ab=\\lambda \\hat\\Rb_{uu}(n-1)$, $\\Bb=\\ub(n)$ and $\\Cb=\\ub(n)^H$, and denoting \n",
    "$$\n",
    "\\Kb_{n+1} = \\left[\\hat\\Rb_{uu}(n+1)\\right]^{-1} \n",
    "$$\n",
    "we readily obtain\n",
    "\\begin{equation}\n",
    "\\Kb(n+1) = \\frac{1}{\\lambda}\\Kb(n) - \n",
    "\\frac{1}{\\lambda^2} \\frac{\\Kb(n)\\ub(n+1)\\ub(n+1)^H \\Kb(n)}{1+\\frac{1}{\\lambda}\\ub(k+1)^H\\Kb(n)\\ub(k+1)},\n",
    "\\end{equation}\n",
    "and after several lines of calculations, we arrive at the updating formula\n",
    "\\begin{equation}\n",
    "\\hat\\wb(n+1) = \\hat\\wb(n) + \\Kb(n+1)\\ub(n+1)[d(n+1)-\\wb(n)^H\\ub(n+1)].\n",
    "\\end{equation}\n",
    "Note that there are some notational differences between the LMS and the RLS. For the LMS, the filter $\\wb(n+1)$ is calculated based on the data available at time $n$. For the RLS,  $\\wb(n+1)$ is computed using data available at time ($n+1$). This is just a notational difference -- we could easily rename $\\wb(n+1)$ into say $\\vb(n)$ and obtain similar indexes. However these notations are traditional, so we follow the classical developments and equations. What is important however is to note that both filters are calculated using the *a priori* error, that is the error using the data at time $n$ and the filter computed using the data at time $n-1$. \n",
    "##### Initialization - \n",
    "The initialization of the algorithm requires the specification of an initial $\\wb(0)$ which is usually taken as a null vector. It also requires specifying $\\Kb(0)$. Since $\\Kb(0)$ is the inverse of the correlation matrix before the beginning of the iterations, we usually choose $\\Rb_{uu}(0)=\\delta\\Ib$, with $\\delta$ very small. So the inverse is $\\Kb(0)=\\delta^{-1}\\Ib$, a large value which disappears during the iterations of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of the RLS algorith is proposed below, using the standard `numpy array` type as well as the `matrix` type. Casting from one type to the other is done by `np.matrix` or `np.array` keywords (which make a copy), or using `np.asmatrix` or `np.asarray` keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implementation using the array type\n",
    "def algo_rls(u,d,M,plambda):\n",
    "    N=size(u)\n",
    "# initialization\n",
    "    e=zeros(N)\n",
    "    wrls=zeros((M,N+1))\n",
    "    Krls=100*eye(M)\n",
    "    u_v=zeros(M)\n",
    "    for n in range(N):\n",
    "        u_v[0]=u[n]\n",
    "        u_v[1:M]=u_v[0:M-1]#concatenate((u[n], u_v[1:M]), axis=0)\n",
    "        e[n]=conj(d[n])-dot(conj(u_v),wrls[:,n])\n",
    "        # print(\"n={}, Erreur de {}\".format(n,e[n]))\n",
    "        Kn=Krls/plambda\n",
    "        Krls=Kn-dot(Kn,dot(outer(u_v,conj(u_v)),Kn))/(1+dot(conj(u_v),dot(Kn,u_v)))\n",
    "        wrls[:,n+1]=wrls[:,n]+dot(Krls,u_v)*conj(e[n])\n",
    "    return (wrls,e)\n",
    "\n",
    "## RLS, matrix version\n",
    "\n",
    "def col(v):\n",
    "     \"\"\" transforms an array into a column vector \\n\n",
    "     This is the equivalent of  x=x(:) under Matlab\"\"\"\n",
    "     v=asmatrix(v.flatten())\n",
    "     return reshape(v,(size(v),1))\n",
    "    \n",
    "def algo_rls_m(u,d,M,plambda):\n",
    "    \"\"\"\n",
    "    Implementation with the matrix type instead of the array type\n",
    "    \"\"\"\n",
    "    N=size(u)\n",
    "\t# initialization\n",
    "    e=zeros(N)\n",
    "    wrls=matrix(zeros((M,N+1)))\n",
    "    Krls=100*matrix(eye(M))\n",
    "    u=col(u)\n",
    "    u_v=matrix(col(zeros(M)))\n",
    "        \n",
    "    for n in range(N):\n",
    "        u_v[0]=u[n]\n",
    "        u_v[1:M]=u_v[0:M-1]\n",
    "\t\t#u_v=concatenate(u[n], u_v[:M], axis=0)\n",
    "        e[n]=conj(d[n])-u_v.H*wrls[:,n]\n",
    "        Kn=Krls/plambda\n",
    "        Krls=Kn-Kn*(u_v*u_v.H*Kn)/(1+u_v.H*Kn*u_v)\n",
    "        wrls[:,n+1]=wrls[:,n]+Krls*u_v*conj(e[n])\n",
    "      \n",
    "    return (wrls,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it would be useful to do again the previous experimentations (identification with non stationary data) with the RLS algorithm. Then to compare and conclude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ident_rls(observation,input_data,factor_lambda=0.95,p=20):\n",
    "    \"\"\" Identification of an impulse response from an observation\n",
    "    `observation` of its output, and from its input `input_data` \\n\n",
    "    `mu` is the adaptation step\\n\n",
    "    Inputs:\n",
    "    =======\n",
    "    observation: array\n",
    "        output of the filter to identify\n",
    "    input_data: array\n",
    "        input of the filter to identify\n",
    "    factor_lambda: real (defaut value=0.95)\n",
    "        forguetting factor in the RLS algorithm\n",
    "    p: int (default =20)\n",
    "        order of the filter    \n",
    "    Outputs:\n",
    "    ========\n",
    "    w: array\n",
    "        identified impulse response\n",
    "    err: array\n",
    "        estimation error\n",
    "    yest: array\n",
    "        estimated output\n",
    "    \"\"\"\n",
    "    N=np.size(input_data)\n",
    "    input_data=squeeze(input_data) #reshape(input_data,(N))\n",
    "    observation=squeeze(observation)\n",
    "    (wrls,e)= algo_rls(input_data,observation,p,factor_lambda)\n",
    "#   (w[:,t+1],erreur[t],yest[t])=lms(input_data[t:t-p:-1],w[:,t],mun)      \n",
    "    return (wrls,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Slow non-stationarity\n",
    "\n",
    "N=1000\n",
    "u=np.random.randn(N)\n",
    "y=np.zeros(N)\n",
    "htest=10*np.array([1, 0.7, 0.7, 0.7, 0.3, 0 ])\n",
    "L=size(htest)\n",
    "for t in np.arange(L,N):\n",
    "    y[t]=dot((1+cos(2*pi*t/N))*htest,u[t:t-L:-1])\n",
    "y+=0.01*np.random.randn(N)\n",
    "plt.figure()\n",
    "plt.plot(y)\n",
    "_=plt.title(\"Observed Signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p=7\n",
    "lamb=0.97\n",
    "(w,err)=ident_rls(y,u,factor_lambda=lamb,p=10)\n",
    "plt.figure(1)\n",
    "clf()\n",
    "plt.plot(err)\n",
    "plt.title('Identification error')\n",
    "figcaption(\"Identification error in the nonstationary case\", label=\"fig:error_ns_case\")\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "t=np.arange(0,N+1)\n",
    "true_ns_h=np.outer((1+cos(2*pi*t/N)),htest)\n",
    "plt.plot(t,w.T,lw=1)\n",
    "plt.plot(t,true_ns_h,lw=2,label=\"True values\", alpha=0.4)\n",
    "plt.title(\"Evolution of filter's coefficients\")\n",
    "figcaption(\"Evolution of filter's coefficients\", label=\"fig:coeff_ns_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "  - http://www.ece.utah.edu/~mathews/ece6550/chapter10.pdf\n",
    "  - http://www.cs.tut.fi/~tabus/course/ASP/LectureNew10.pdf\n",
    "  - [Recursive Least Squares at wikipedia](http://en.wikipedia.org/wiki/Recursive_least_squares_filter)\n",
    "  - <a href=\"http://www.intechopen.com/books/adaptive-filtering-applications\" title=\"Adaptive Filtering Applications\">Adaptive Filtering Applications</a> (open access book at intechopen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<div align=right> [Index](toc.ipynb) - [Back](Grad_algo.ipynb) - [Next](noisecancellationlab.ipynb)</div>"
   ]
  }
 ],
 "metadata": {
  "interactive_sols": {
   "cbx_id": 3
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "<div class=\"lev2\"><a href=\"#Introduction-to-Random-Signals\">Introduction to Random Signals</a></div><div class=\"lev4\"><a href=\"#Notations\">Notations</a></div><div class=\"lev2\"><a href=\"#Fundamental-properties\">Fundamental properties</a></div><div class=\"lev3\"><a href=\"#Stationnarity\">Stationnarity</a></div><div class=\"lev3\"><a href=\"#Ergodism\">Ergodism</a></div><div class=\"lev4\"><a href=\"#Definition\">Definition</a></div><div class=\"lev3\"><a href=\"#Examples-of-random-signals\">Examples of random signals</a></div><div class=\"lev3\"><a href=\"#White-noise\">White noise</a></div><div class=\"lev2\"><a href=\"#Second-order-analysis\">Second order analysis</a></div><div class=\"lev3\"><a href=\"#Correlation-functions\">Correlation functions</a></div><div class=\"lev4\"><a href=\"#Definition\">Definition</a></div><div class=\"lev4\"><a href=\"#Main-properties\">Main properties</a></div><div class=\"lev4\"><a href=\"#Exercises\">Exercises</a></div><div class=\"lev4\"><a href=\"#Estimation-of-correlation-functions\">Estimation of correlation functions</a></div><div class=\"lev4\"><a href=\"#Detecting-hidden-periodicities\">Detecting hidden periodicities</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Configuring matplotlib formats\n",
      "... Configuring matplotlib with inline figures\n",
      "... Importing numpy as np, scipy as sp, pyplot as plt, scipy.stats as stats\n",
      "   ... scipy.signal as sig\n",
      "... Importing widgets, display, HTML, Image, Javascript\n",
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\require{color}\n",
       "\\require{cancel}\n",
       "\\def\\tf#1{{\\mathrm{FT}\\left\\{ #1 \\right\\}}}\n",
       "\\def\\flecheTF{\\rightleftharpoons }\n",
       "\\def\\TFI#1#2#3{{\\displaystyle{\\int_{-\\infty}^{+\\infty} #1 ~e^{j2\\pi #2 #3} \n",
       "~\\dr{#2}}}}\n",
       "\\def\\TF#1#2#3{{\\displaystyle{\\int_{-\\infty}^{+\\infty} #1 ~e^{-j2\\pi #3 #2} \n",
       "~\\dr{#2}}}}\n",
       "\\def\\sha{Ñˆ}\n",
       "\\def\\dr#1{\\mathrm{d}#1}\n",
       "\\def\\egalpardef{\\mathop{=}\\limits^\\triangle}\n",
       "\\def\\sinc#1{{\\mathrm{sinc}\\left( #1 \\right)}}\n",
       "\\def\\rect{\\mathrm{rect}}\n",
       "\\definecolor{lightred}{rgb}{1,0.1,0}\n",
       "\\def\\myblueeqbox#1{{\\fcolorbox{blue}{lightblue}{$\textcolor{blue}{ #1}$}}}\n",
       "\\def\\myeqbox#1#2{{\\fcolorbox{#1}{light#1}{$\textcolor{#1}{ #2}$}}}\n",
       "\\def\\eqbox#1#2#3#4{{\\fcolorbox{#1}{#2}{$\\textcolor{#3}{ #4}$}}}\n",
       "% border|background|text\n",
       "\\def\\eqboxa#1{{\\boxed{#1}}}\n",
       "\\def\\eqboxb#1{{\\eqbox{green}{white}{green}{#1}}}\n",
       "\\def\\eqboxc#1{{\\eqbox{blue}{white}{blue}{#1}}}\n",
       "\\def\\eqboxd#1{{\\eqbox{blue}{lightblue}{blue}{#1}}}\n",
       "\\def\\E#1{\\mathbb{E}\\left[#1\\right]}\n",
       "\\def\\ta#1{\\left<#1\\right>}\n",
       "\\def\\egalparerg{{\\mathop{=}\\limits_\\mathrm{erg}}}\n",
       "\\def\\expo#1{\\exp\\left(#1\\right)}\n",
       "\\def\\d#1{\\mathrm{d}#1}\n",
       "\\def\\wb{\\mathbf{w}} \n",
       "\\def\\sb{\\mathbf{s}} \n",
       "\\def\\xb{\\mathbf{x}}\n",
       "\\def\\Rb{\\mathbf{R}} \n",
       "\\def\\rb{\\mathbf{r}} \n",
       "\\def\\mystar{{*}}\n",
       "\\def\\ub{\\mathbf{u}}\n",
       "\\def\\wbopt{\\mathop{\\mathbf{w}}\\limits^\\triangle}\n",
       "\\def\\deriv#1#2{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n",
       "\\def\\Ub{\\mathbf{U}}\n",
       "\\def\\db{\\mathbf{d}}\n",
       "\\def\\eb{\\mathbf{e}}\n",
       "\\def\\vb{\\mathbf{v}}\n",
       "\\def\\Ib{\\mathbf{I}}\n",
       "\\def\\Vb{\\mathbf{V}}\n",
       "\\def\\Lambdab{\\mathbf{\\Lambda}}\n",
       "\\def\\Ab{\\mathbf{A}}\n",
       "\\def\\Bb{\\mathbf{B}}\n",
       "\\def\\Cb{\\mathbf{C}}\n",
       "\\def\\Db{\\mathbf{D}}\n",
       "\\def\\Kb{\\mathbf{K}}\n",
       "\\def\\sinc#1{\\mathrm{sinc\\left(#1\\right)}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Defining figures captions \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".caption {\n",
       "font-weight: normal;\n",
       "text-align: left;\n",
       "width:60%; margin-left:10%; border:2px solid; padding-top:5px; padding-bottom:5px;\n",
       "background-color:white;border-color:#efd3d7;color:black;\n",
       "border-radius:8px;-webkit-border-radius:8px;-moz-border-radius:8px;border-radius:8px\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loading customized Javascript for interactive solutions (show/hide)\n",
      "... Redefining interactive from ipywidgets\n"
     ]
    }
   ],
   "source": [
    "%run nbinit.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some specific imports for plotting\n",
    "\n",
    "from plot_rea import *\n",
    "from plot_sighisto import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Random Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a random variable is a set of values associated with a probability distribution, a random signal, also callled a random process, is a set of *functions* associated with a probability distribution. In addition to properties similar to those of random variables, the study of random processes include characterizations of dependences *in* the random process (namely notions of *correlation*), the study the behaviour of the function under transformations (*filtering*) and the design of some optimal transformations.\n",
    "\n",
    "#### Notations\n",
    "We denote by $X(n,\\omega)$ a random signal $X$. It is a set of functions of $n$, the set being indexed by  $\\omega$. A random signal is thus a bivariate quantity. When  $\\omega=\\omega_i$ is fixed, we get a *realization* of the random process, denoted\n",
    "$X(n,\\omega_i)$ or, more simply $x_i(n)$.\n",
    "When $n$ is fixed, the random process reduces to a simple random variable. Considering the process for $n=n_i$, \n",
    "we obtain a random variable $X(n_i,\\omega)$, denoted\n",
    "$X_i(\\omega)$, or $X_i$. Finally, we will denote $x_i$ the values taken by the random variable\n",
    "$X_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationnarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{definition}\n",
    "A random signal is said stationnary if its statistical properties are invariant by time translation. \n",
    "\\end{definition}\n",
    "\n",
    "This means that the joint probability density function\n",
    "$$\n",
    "p_{X(n_1), X(n_2), \\ldots, X(n_k)} = p_{ X(n_1-\\tau), X(n_2-\\tau),\n",
    "\\ldots, X(n_k-\\tau)},\n",
    "$$\n",
    "and if $\\tau=n_k$\n",
    "$$\n",
    "p_{X(n_1), X(n_2), \\ldots, X(n_k)} = p_{ X(n_1-n_k), X(n_2-n_k), \n",
    "\\ldots, X(0)}.\n",
    "$$\n",
    "Therefore, the joint distribution only depends on $k-1$ parameters, instead\n",
    "of the  $k$ initial parameters.\n",
    "\n",
    "As a consequence, we have that\n",
    "\n",
    "- $\\E{X(n)}=\\mu$  is constant and does not depend on the particular time $n$\n",
    "- $\\E{X(n)X(n-\\tau)^*}=R_X(\\tau)$  only depends on the delay between the two instants. In such a case, the resulting function $R_X(\\tau)$ is called a **correlation function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\ta#1{\\left<#1\\right>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ergodism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time average, taken on realization $\\omega$ is \n",
    "$$\n",
    "\\ta{X(n,\\omega)^n} = \\lim_{N \\rightarrow +\\infty} \\frac{1}{N} \\sum_{[N]} \n",
    "X(n,\\omega)^n.\n",
    "$$\n",
    "Of course, in the general case, this time average **is** a random varaible, since it depends on $\\omega$.\n",
    "\n",
    "**Definition**\n",
    "A random signal is said ergodic if its time averages are deterministic, i.e. non random, variables.\n",
    "\n",
    "**Important consequence**\n",
    "\n",
    "A really important consequence is that if a signal is both stationnary and ergodic, then the statistical means and the time averages are equal. \n",
    "$$\n",
    "\\boxed{\\E\\bullet=\\ta\\bullet}\n",
    "$$\n",
    "\n",
    "**Exercise**\n",
    "> - Check that \n",
    "\n",
    "- (moments)\n",
    "Check that if the signal is  both stationnary and ergodic, then\n",
    "$$\n",
    "\\E{X(n,\\omega)^n} = \\lim_{N \\rightarrow +\\infty} \\frac{1}{N} \n",
    "\\sum_{[N]} X(n,\\omega)^n,\n",
    "$$\n",
    "\n",
    "- (covariance) \n",
    "Similarly, check that\n",
    "$$\n",
    "R_X(\\tau) = \\E{X(n,\\omega)X(t-\\tau,\\omega)} = \\lim_{N \\rightarrow +\\infty} \n",
    "\\frac{1}{N} \\sum_{[N]} X(n,\\omega)X(t-\\tau,\\omega).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of random signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1.  Let us first consider the noisy sine wave $X(n,\\omega) = A\\sin(2\\pi f_0 n)+B(n,\\omega)$. Function `plot_rea` plots some realizations of this signal and plots the ensemble and time averages. You will also need `sig_histo` which plots the histogram, together with the time series.\n",
    ">```\n",
    "      from plot_rea import *\n",
    "      from plot_sighisto import *\n",
    "```\n",
    "Experiment with the parameters (amplitude, number of samples). Is the signal stationary, ergodic, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "M=10; # number of bins in histograms\n",
    "N=1500 # Number of samples per realization\n",
    "K=200 # Total number of realizations\n",
    "\n",
    "XGauss=stats.norm(loc=0,scale=1) \n",
    "\n",
    "#Sine wave plus noise\n",
    "X=3*XGauss.rvs(size=(K,N))+3*np.outer(np.ones((K,1)),np.sin(2*np.pi*np.arange(N)/N))\n",
    "print(\"Standard deviation of time averages: \",np.std(np.mean(X,axis=1)))\n",
    "#pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plot_rea(X,nb=10,fig=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the number of samples $N$, we see that the time average converges to zero, for each realization. Thus we could say that this process is ergodic. However, the ensemble average converges to the sine wave and is dependent if time: the process *is not stationary*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XGauss=stats.norm(loc=0,scale=1) \n",
    "#pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "\n",
    "def q1_experiment(N):\n",
    "    K=200\n",
    "    #Sine wave plus noise\n",
    "    X=3*XGauss.rvs(size=(K,N))+3*np.outer(np.ones((K,1)),np.sin(2*np.pi*np.arange(N)/N))\n",
    "    print(\"Standard deviation of time averages: \",np.std(np.mean(X,axis=1)))\n",
    "    plot_rea(X,nb=10,fig=1)\n",
    "interact(q1_experiment, N=(0,2000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    ">  2- Consider now a sine wave with a random phase $X(n,\\omega) = A\\sin(2 \\pi f_0 n +\\phi\u001e",
    "\n",
    "(\\omega))$. \n",
    "\n",
    "Experiment with the parameters (amplitude, number of samples). Is the signal stationary, ergodic, etc? Also change the value of the frequency, and replace function `sin` by `square` which generates a pulse train instead of a sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "K=100\n",
    "N=1000\n",
    "fo=2.2/N\n",
    "S=zeros((K,N))\n",
    "for r in range(K):\n",
    "    S[r,:]=1.1*sin(2*pi*fo*arange(N) + 2*pi*rand(1,1));\n",
    "plot_rea(S,fig=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This example shows that a random signal is not necessarily noisy and irregular. Here we have a random signal which is `smooth'. The random character is introduced by the random phase, which simply reflects that we do not know the time origin of this sine wave. \n",
    "\n",
    "Here, we see that both the time average and the ensemble average converge to zero. Therefore, we can conclude that this signal is stationary and ergodic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a square wave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    \"\"\"square(x): \\n\n",
    "    Returns a pulse train with period :math:`2\\pi`\n",
    "    \"\"\"\n",
    "    return sign(sin(x))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate a random square wave as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K=1000\n",
    "N=1000\n",
    "S=zeros((K,N))\n",
    "for r in range(K):\n",
    "    S[r,:]=1.1*square(2*pi*fo*arange(N) + 2*pi*rand(1,1));\n",
    "plot_rea(S,fig=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Again, we see that both means tend to zero, a constant, which means that the signal is stationary (its ensemble average does not depend of time) and ergodic (its time average does not depend on the actual realization). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete signals, a white noise is simply a sequence of independent variables (often the variables will be also identically distributed). An independent and identically distributed signal is denoted iid. \n",
    "Since the components of a white noise are all independent, there will be no correlation between them. We will see later that the spectral representation of a white noise is *flat*, thus coining the name of white noise by analogy with the white light. \n",
    "\n",
    "The notion of white noise is more involved in the case of a time-continuous signal. The white noise is in such case a limit processe with \"microscopic dependences\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We consider now two kinds of random noises: the first one is a sequence of independent and identically distributed variables (iid variables), according to a uniform distribution. The second one is an iid sequence, Gaussian distributed. Plot the two probability density functions, plot the histograms (with `sig_histo`) and compare the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">3- Compute and analyze the histograms of two white noises, respectively with a uniform and a Gaussian probability density function, using the lines in script `q1c`. Do this for several realizations (launch the program again and again) and change the number of points and of bins. Compare the two signals. What do you think of the relation between whiteness and gaussianity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An object \"uniform random variable\" with fixed bounds [0,1]\n",
    "x_uni=stats.uniform(loc=0,scale=1)\n",
    "# An object \"gaussian random variable\" with zero mean and scale 1\n",
    "x_gauss=stats.norm(loc=0,scale=1)\n",
    "#plt.rcParams['figure.figsize'] = (8,5)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,3))\n",
    "x=arange(-3,3,0.1)\n",
    "ax1.plot(x,x_uni.pdf(x)); ax1.set_ylim([0, 1.1*max(x_uni.pdf(x))])\n",
    "ax2.plot(x,x_gauss.pdf(x)); ax2.set_ylim([0, 1.1*max(x_gauss.pdf(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(m,v)=x_uni.stats(moments='mv')\n",
    "print(\"Uniform distribution: \",\"Value of the mean : {0:2.3f} and of the variance {1:2.3f}\".format(float(m),float(v)))\n",
    "(m,v)=x_gauss.stats(moments='mv')\n",
    "print(\"Gauss distribution: \",\"Value of the mean : {0:2.3f} and of the variance {1:2.3f}\".format(float(m),float(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the two signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2)=subplots(2,1,sharex=True)\n",
    "ax1.plot(x_uni.rvs(size=N))\n",
    "ax2.plot(x_gauss.rvs(size=N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_sighisto import *\n",
    "plot_sighisto(x_uni.rvs(size=N),fig=1)\n",
    "plot_sighisto(x_gauss.rvs(size=N),fig=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Gaussian noise is more concentrated on its mean 0, and exhibits more important values, while the uniform noise is confined into the interval [0,1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the question on the relation between whiteness and Gaussianity, actually, there is no relation between these two concepts. A white noise can be distributed according to any distribution, and a Gaussian sequence is not necessarily iid (white). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X(n,\\omega)$ and $Y(n,\\omega)$ sare two jointly stationnary random processes, the intercorrelation and autocorrelation functions are defined by\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\displaystyle{R_{XY}(k) \\egalpardef \\E{X(n,\\omega)Y^*(n-k,\\omega)} \\egalparerg \n",
    "\\lim_{N \\rightarrow +\\infty}\\frac{1}{N} \\sum_{n=0}^N X(n,\\omega)Y^*(n-k,\\omega), }\n",
    "\\\\ \n",
    "\\displaystyle{R_{XX}(k) \\egalpardef \\E{X(n,\\omega)X^*(n-k,\\omega)} \\egalparerg \n",
    "\\lim_{N \\rightarrow +\\infty}\\frac{1}{N} \\sum_{n=0}^N X(n,\\omega)X^*(n-k,\\omega)}.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{enumerate}\n",
    "\\item (Hermitian symmetry)\n",
    "$$\n",
    "R_{YX}(\\tau)=\\E{Y(n,\\omega)X^\\star(n-\\tau,\\omega)} = \\E{Y(n+\\tau,\\omega)X^\\star(n,\\omega)}= \\E{X(n,\\omega)Y^\\star(n+\\tau,\\omega)}^\\star = R_{XY}^\\star(-\\tau).\n",
    "$$\n",
    "\n",
    "\\item (Symmetry for the autocorrelation). In the case of the autocorrelation function, the hermitian symmetry reduces to\n",
    "$$\n",
    "R_{XX}(\\tau)= R_{XX}^\\star(-\\tau).\n",
    "$$\n",
    "\\item (Centering). If \n",
    "$X_c(n,\\omega)=X(n,\\omega) - m_X$ is the centered signal, then \n",
    "$$\n",
    "R_{XX}(\\tau) = R_{X_cX_c}(\\tau)+m_X^2.\n",
    "$$\n",
    "\\item (Autocorrelation and power).  For a delay $\\tau=0$, we have\n",
    "$$\n",
    "R_{XX}(0) = \\E{|X(n,\\omega)|^2} \\egalparerg \\lim_{N \\rightarrow \n",
    "+\\infty}\\frac{1}{N} \\sum_{[N]} |X(n,\\omega)|^2  = P_X.\n",
    "$$\n",
    "This shows that $R_{XX}(0)$ is nothing but the power of the signal under study. Observe that necessarily $R_{XX}(0)>0$.\n",
    "\n",
    "\\item (Maximum). Beginning with the _Schwarz inequality_, \n",
    "$$\n",
    "|<x,y>|^2 \\leq <x,x> <y,y>,\n",
    "$$\n",
    "and using the scalar product $<x_1,x_2> = \\E{X_1(n)X_2^*(n)}$, we get\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item  $|R_{YX}(\\tau)|^2 \\leq R_{XX}(0)R_{YY}(0), ~~~~ \\forall  \\tau,$\n",
    "\\item  $|R_{XX}(\\tau)| \\leq R_{XX}(0), ~~~~ \\forall  \\tau, $\n",
    "\\end{itemize}\n",
    "\n",
    "\\item (Non negativity) The autocorrelation function is non negative definite\n",
    "$$\n",
    "\\sum_i \\sum_j \\lambda_i R_{XX}(\\tau_i - \\tau_j) \\lambda_j \\geq 0,  ~~~\n",
    "\\forall i,j. \n",
    "$$\n",
    "proof: develop  $\\E{|\\sum_i \\lambda_i X(\\tau_i)|^2}\\geq0$\n",
    "\n",
    "\\item (Correlation coefficient). By the maximum property, the correlation coefficient\n",
    "$$\n",
    "\\rho_{XY}(\\tau)=\\frac{R_{YX}(\\tau)}{\\sqrt{R_{XX}(0)R_{YY}(0)}}\n",
    "$$\n",
    "is such that $\\rho_{XY}(\\tau) \\leq 1$.\n",
    "\\item (Memory). If the correlation coefficient is zero after a certain time\n",
    " $t_c$ then the process is said to have a finite memory and $t_c$ is called the *correlation time*. \n",
    "\\end{enumerate} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "\\begin{enumerate}\n",
    "\\item Developing $\\E{|X+ \\lambda Y|^2}$ into a polynom of $\\lambda$ and observing that this polynom in always nonegative, prove the Schwarz inequality. \n",
    "\n",
    "\\item Consider a random signal $U(n,\\omega)$ defined on the interval\n",
    "$[0,N]$. Define the periodic signal \n",
    "$$\n",
    "X(n,\\omega)=\\mathrm{Rep}_N[U(n,\\omega)] =\\sum_k U(t-kN,\\omega).\n",
    "$$\n",
    "\\begin{itemize}\n",
    " \\item Show that $R_{UU}(\\tau)=0$ for $\\tau \\notin [-N,N]$.\n",
    " \\item Show that  $R_{XX}(\\tau)$ is a periodic function with period $N$ and express  $R_{XX}(\\tau)$ as a function of $R_{UU}(\\tau)$. \n",
    "\\end{itemize}\n",
    "\n",
    "\\item Consider a random signal $X(n,\\omega)$ with autocorrelation  $R_{XX}(k)$ and define\n",
    "$$\n",
    "Z(n,\\omega) = X(n,\\omega) + a X(n-n_0,\\omega).\n",
    "$$\n",
    "Compute the autocorrelation function of $Z(n,\\omega)$.\n",
    "\\end{enumerate}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of correlation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  By stationnarity and ergodism, we have that\n",
    "  $$\n",
    "  \\displaystyle{R_{XX}(k)  \\egalparerg \n",
    "\\lim_{N \\rightarrow +\\infty}\\frac{1}{N} \\sum_{n=0}^N X(n,\\omega)X^*(n-k,\\omega)}.\n",
    "  $$\n",
    "Given a finite number of points $N$, with data known from $n=0..N-1$, it is thus possible to approximate the correlation function by a formula like\n",
    "$$\n",
    "\\displaystyle{R_{XX}(k)  =\n",
    "\\frac{1}{N} \\sum_{n=0}^{N-1} X(n,\\omega)X^*(n-k,\\omega)}.\n",
    "$$\n",
    "If we take $k\\geq0$, we see that $X^*(n-k,\\omega)$ is unavailable for $k>n$. Consquently, the sum must go from $n=k$ to $N-1$. At this point, people define two possible estimators. The first one is said \"unbiased\" while the second is \"biased\" (check this by computing the expectation $\\E{\\bullet}$ of the two estimators). \n",
    "\\begin{align}\n",
    "\\hat{R}_{XX}^{(unbiased)}(k)   &=\n",
    "\\frac{1}{N-k} \\sum_{n=k}^{N-1} X(n,\\omega)X^*(n-k,\\omega) \\\\\n",
    "\\hat{R}_{XX}^{(biased)}(k)  &=\n",
    "\\frac{1}{N} \\sum_{n=k}^{N-1} X(n,\\omega)X^*(n-k,\\omega).\n",
    "\\end{align}\n",
    "\n",
    "For the biased estimator, it can be shown (Bartlett) that the variance has the form\n",
    "$$\n",
    "Var\\left[\\hat{R}_{XX}^{(biased)}(k)\\right]  \\approx \\frac{1}{N} \\sum_{m=-\\infty}^{+\\infty} \\rho(m)^2 + \\rho(m+k)\\rho(m-k) -4\\rho(m)\\rho(k)\\rho(m-k) + 2\\rho(m)^2\\rho(k)^2,\n",
    "$$\n",
    "that is, essentially a constant over $N$. As far the unbiased estimator is concerned, we will have a factor $N/(N-k)$, and we see that this time the variance *increases* with $k$. Thus, though it is unbiased, this estimator has a very bad behaviour with respect to the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is checked below. First we generate a gaussian white noise, compute the two estimates of the correlation function and copare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from correlation import xcorr\n",
    "from scipy import stats as stats\n",
    "N=100\n",
    "XGauss=stats.norm(loc=0,scale=1) \n",
    "S=XGauss.rvs(size=N)\n",
    "#\n",
    "Rbiased,lags=xcorr(S,norm='biased')\n",
    "Runbiased,lags=xcorr(S,norm='unbiased')\n",
    "Rtheo=zeros(size(Rbiased))\n",
    "Rtheo[lags==0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rt=ones(1)\n",
    "fig,ax=subplots(3,1,figsize=(7,7),sharex=True,sharey=True)\n",
    "# biased correlation\n",
    "ax[1].plot(lags,Rbiased)\n",
    "#ax[0].axvline(0,ymin=0,ymax=1,color='r',lw=3)\n",
    "ax[1].set_title(\"Biased Correlation function\")\n",
    "ax[1].set_xlabel(\"Delay\")\n",
    "ax[1].axis('tight')  #Tight layout of the axis\n",
    "# unbiased correlation\n",
    "ax[2].plot(lags,Runbiased)\n",
    "ax[2].set_title(\"Unbiased Correlation function\")\n",
    "ax[2].set_xlabel(\"Delay\")\n",
    "# theoretical correlation\n",
    "ax[0].stem([0],[1],linefmt='r-', markerfmt='ro', basefmt='r-')\n",
    "ax[0].plot([lags[0],lags[-1]],[0, 0],'r')\n",
    "ax[0].set_title(\"True Correlation function\")\n",
    "fig.tight_layout()\n",
    "ax[1].axis('tight')\n",
    "ax[0].set_ylim([-0.5,1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting hidden periodicities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider here a time series composed of a periodic signal corrupted by a white noise. \n",
    "The signal is completely hidden by the noise. We show here that it is possible to find some information in the autocorrelation function.\n",
    "\n",
    "**Exercises** \n",
    " - (a) Check that the correlation of a periodic signal is periodic \n",
    " - (b) Give the correlation of $y=s+w$ if $s$ and $w$ are independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    \"\"\"square(x): \\n\n",
    "    Returns a pulse train with period :math:`2\\pi`\n",
    "    \"\"\"\n",
    "    return sign(sin(x))   \n",
    "\n",
    "N=1000\n",
    "f0=0.05\n",
    "t=np.linspace(0,400,N)\n",
    "x=square(2*pi*f0*t)\n",
    "noise=stats.norm(loc=0,scale=2).rvs(N)\n",
    "observation=x+noise\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Plot the correlation of the noisy signal. Are you able to retrieve the unknown periodicities? Experiment with the parameters. Conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "plt.plot(t,x,'-')\n",
    "plt.plot(t,observation)\n",
    "#\n",
    "Rbiased,lags=xcorr(observation,norm='biased',maxlags=100)\n",
    "plt.figure()\n",
    "plt.plot(lags,Rbiased)\n",
    "plt.grid(b='on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "source": [
    "The last figure shows the correlation of the noisy periodic signal. This correlation is simply the superposition of the correlation of the noise and of the correlation of the signal (Check it!)\n",
    "$$\n",
    "R_\\text{obs,obs}=R_\\text{sig,sig}+R_\\text{noise,noise}\n",
    "$$\n",
    "Since the correlation of the noise (a Dirac impulse) is concentrated at zero, we can read\n",
    "- the period of the signal: 50 (that is a relative frequency of 50/1000=0.05)\n",
    "- the power of the signal: 1\n",
    "- the power of the noise: 5 - 1 = 4 (was generated with a standard deviation of 2). \n",
    "The correlation function then enable us to grasp many informations that were not apparent in the time series!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "<div align=right> <a href=toc.ipynb>Index</a> - <a href=BasicFiltering_correction.ipynb>Back</a> - <a href=Lecture2_RandomSignals.ipynb>Next</a></div>"
   ]
  }
 ],
 "metadata": {
  "interactive_sols": {
   "cbx_id": 4
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc-wrapper_display": "none",
   "toc_cell": true,
   "toc_number_sections": false,
   "toc_threshold": 4
  },
  "toc_position": {
   "left": "1290.9px",
   "right": "20px",
   "top": "120px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

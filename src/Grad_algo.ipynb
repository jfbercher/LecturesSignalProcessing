{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2\"><a href=\"#The-steepest-descent-algorithm\">The steepest descent algorithm</a></div><div class=\"lev2\"><a href=\"#Application-to-the-iterative-resolution-of-the-normal-equations\">Application to the iterative resolution of the normal equations</a></div><div class=\"lev3\"><a href=\"#Convergence-analysis\">Convergence analysis</a></div><div class=\"lev4\"><a href=\"#Conditions-on-the-step-size\">Conditions on the step-size</a></div><div class=\"lev4\"><a href=\"#Optimum-step-size\">Optimum step-size</a></div><div class=\"lev3\"><a href=\"#An-alternative-view-of-the-Steepest-Descent-Algorithm\">An alternative view of the Steepest Descent Algorithm</a></div><div class=\"lev4\"><a href=\"#The-sum-of-$n$-terms-of-a-geometric-series-of-matrices\">The sum of $n$ terms of a geometric series of matrices</a></div><div class=\"lev4\"><a href=\"#An-iterative-formula-for-computing-the-solution-of-the-normal-equation\">An iterative formula for computing the solution of the normal equation</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Configuring matplotlib formats\n",
      "... Configuring matplotlib with inline figures\n",
      "... Importing numpy as np, scipy as sp, pyplot as plt, scipy.stats as stats\n",
      "   ... scipy.signal as sig\n",
      "... Importing widgets, display, HTML, Image, Javascript\n",
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\require{color}\n",
       "\\require{cancel}\n",
       "\\def\\tf#1{{\\mathrm{FT}\\left\\{ #1 \\right\\}}}\n",
       "\\def\\flecheTF{\\rightleftharpoons }\n",
       "\\def\\TFI#1#2#3{{\\displaystyle{\\int_{-\\infty}^{+\\infty} #1 ~e^{j2\\pi #2 #3} \n",
       "~\\dr{#2}}}}\n",
       "\\def\\TF#1#2#3{{\\displaystyle{\\int_{-\\infty}^{+\\infty} #1 ~e^{-j2\\pi #3 #2} \n",
       "~\\dr{#2}}}}\n",
       "\\def\\sha{ш}\n",
       "\\def\\dr#1{\\mathrm{d}#1}\n",
       "\\def\\egalpardef{\\mathop{=}\\limits^\\triangle}\n",
       "\\def\\sinc#1{{\\mathrm{sinc}\\left( #1 \\right)}}\n",
       "\\def\\rect{\\mathrm{rect}}\n",
       "\\definecolor{lightred}{rgb}{1,0.1,0}\n",
       "\\def\\myblueeqbox#1{{\\fcolorbox{blue}{lightblue}{$\textcolor{blue}{ #1}$}}}\n",
       "\\def\\myeqbox#1#2{{\\fcolorbox{#1}{light#1}{$\textcolor{#1}{ #2}$}}}\n",
       "\\def\\eqbox#1#2#3#4{{\\fcolorbox{#1}{#2}{$\\textcolor{#3}{ #4}$}}}\n",
       "% border|background|text\n",
       "\\def\\eqboxa#1{{\\boxed{#1}}}\n",
       "\\def\\eqboxb#1{{\\eqbox{green}{white}{green}{#1}}}\n",
       "\\def\\eqboxc#1{{\\eqbox{blue}{white}{blue}{#1}}}\n",
       "\\def\\eqboxd#1{{\\eqbox{blue}{lightblue}{blue}{#1}}}\n",
       "\\def\\E#1{\\mathbb{E}\\left[#1\\right]}\n",
       "\\def\\ta#1{\\left<#1\\right>}\n",
       "\\def\\egalparerg{{\\mathop{=}\\limits_\\mathrm{erg}}}\n",
       "\\def\\expo#1{\\exp\\left(#1\\right)}\n",
       "\\def\\d#1{\\mathrm{d}#1}\n",
       "\\def\\wb{\\mathbf{w}} \n",
       "\\def\\sb{\\mathbf{s}} \n",
       "\\def\\xb{\\mathbf{x}}\n",
       "\\def\\Rb{\\mathbf{R}} \n",
       "\\def\\rb{\\mathbf{r}} \n",
       "\\def\\mystar{{*}}\n",
       "\\def\\ub{\\mathbf{u}}\n",
       "\\def\\wbopt{\\mathop{\\mathbf{w}}\\limits^\\triangle}\n",
       "\\def\\deriv#1#2{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n",
       "\\def\\Ub{\\mathbf{U}}\n",
       "\\def\\db{\\mathbf{d}}\n",
       "\\def\\eb{\\mathbf{e}}\n",
       "\\def\\vb{\\mathbf{v}}\n",
       "\\def\\Ib{\\mathbf{I}}\n",
       "\\def\\Vb{\\mathbf{V}}\n",
       "\\def\\Lambdab{\\mathbf{\\Lambda}}\n",
       "\\def\\Ab{\\mathbf{A}}\n",
       "\\def\\Bb{\\mathbf{B}}\n",
       "\\def\\Cb{\\mathbf{C}}\n",
       "\\def\\Db{\\mathbf{D}}\n",
       "\\def\\Kb{\\mathbf{K}}\n",
       "\\def\\sinc#1{\\mathrm{sinc\\left(#1\\right)}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Defining figures captions \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".caption {\n",
       "font-weight: normal;\n",
       "text-align: left;\n",
       "width:60%; margin-left:10%; border:2px solid; padding-top:5px; padding-bottom:5px;\n",
       "background-color:white;border-color:#efd3d7;color:black;\n",
       "border-radius:8px;-webkit-border-radius:8px;-moz-border-radius:8px;border-radius:8px\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loading customized Javascript for interactive solutions (show/hide)\n",
      "... Redefining interactive from ipywidgets\n"
     ]
    }
   ],
   "source": [
    "%run nbinit.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steepest descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although direct inversion provide the solution in a finite number of steps, it is sometimes preferable to use alternative iterative methods because they may require less numerical precision, are usually computationally and can even be applied in the case of non-quadratic\n",
    "criteria. The adaptive filtering algorithms we will see later will be obtained by simple modifications of iterative methods. Before indicating how such methods can be useful for solving our normal equations, we begin by describing the Steepest Descent Algorithm (SDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(\\xb)$ be a differentiable function of $\\xb$ with continuous derivatives. It is then possible to approximate the function at a point $\\xb+\\Delta\\xb$ using the Taylor expansion\n",
    "$$\n",
    "f(\\xb+\\Delta\\xb) = f(\\xb)+\\Delta\\xb^T\\nabla f(\\xb) + \\frac{1}{2} \\Delta\\xb^T\\nabla^2 f(\\xb)\\Delta\\xb + \\ldots,\n",
    "$$\n",
    "where $\\nabla f(\\xb)$ denotes the gradient of $f$ at $\\xb$ and $\\nabla^2 f(\\xb)$ the Hessian. Restricting ourselves to the first order approximation, we see that if we choose $\\Delta\\xb^T\\nabla f(\\xb)<0$, then $f(\\xb+\\Delta\\xb) < f(\\xb)$, i.e. $f$ decreases. The higher $|\\Delta\\xb^T\\nabla f(\\xb)|$, the most important the decrease.  The scalar product is maximum when the two vectors are colinear, and they must have opposite direction so as to obtain a negative scalar product. This yields \n",
    "$$\n",
    "\\Delta\\xb = -\\nabla f(\\xb).\n",
    "$$\n",
    "The negative of the gradient is known as the direction of steepest descent. Usually, to keep $\\Delta\\xb$ small enough for the validity of the Taylor approximation, one uses a small positive factor $\\mu$ in front of the gradient. This leads to the following iterative algorithm\n",
    "\\begin{equation}\n",
    "\\eqboxc{\n",
    "\\xb_{k+1} = \\xb_{k} -\\mu \\nabla f(\\xb)},\n",
    "\\label{eq:grad_algo}\n",
    "\\end{equation}\n",
    "which is known as the \\textem{steepest descent algorithm}. We begin with an initial guess $\\xb_{0}$ of the solution and take the gradient of the function at that point. Then we update the solution in the negative direction of the gradient and we repeat the process until the algorithm  eventually converges where the gradient is zero. Of course, this works if the function at hands possesses a true minimum, and even in that case, the solution may correspond to a local minimum. In addition, the value of the step-size $\\mu$ can be crucial for the actual convergence and the speed of convergence to a minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give below a simple implementation of a steepest descent algorithm. Beyond formula (\\ref{eq:grad_algo}), we have refined by \n",
    "\n",
    "- specifying a stopping rule: error less than a given precision `err` or number of iteration greater than a maximum number of iterations `itermax`\n",
    "- a line-search procedure `line_search` (True by default) which adapts the step-size in order to ensure that the objective function actually decreases\n",
    "- a verbose mode `verbose`  (True by default) which prints some intermediary  results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some references available online: \n",
    "\n",
    "- [Gradient descent](http://en.wikipedia.org/wiki/Gradient_descent)\n",
    "- [Gradient desxcent (2)](http://www.onmyphd.com/?p=gradient.descent)\n",
    "- [Conjugate gradients](http://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(f, g, mu, x0=0, eps=0.001, grad_prec=0.0001, itermax=200, line_search=True, verbose=True):\n",
    "    \n",
    "    def update_grad(xk,mu):\n",
    "        return xk-mu*g(xk)\n",
    "    \n",
    "    xk=np.zeros((np.size(x0),itermax))\n",
    "    xk[:,0]=x0\n",
    "    err=1\n",
    "    k=0\n",
    "    while err>eps and k<itermax-1:\n",
    "        err=norm(xk[:,k]-update_grad(xk[:,k],mu),1)\n",
    "        xk[:,k+1]=update_grad(xk[:,k],mu)\n",
    "        if (np.any(np.isnan(xk[:,k+1])) or np.any(np.isinf(xk[:,k+1])) ): break\n",
    "        m=0\n",
    "        #line search: look for a step that ensures that the objective function decreases\n",
    "        if line_search:\n",
    "            while f(xk[:,k+1])>f(xk[:,k]):\n",
    "                #print(\"Updating..\",  f(xk[k+1]),f(xk[k]))\n",
    "                m=m+1\n",
    "                xk[:,k+1]=update_grad(xk[:,k],mu*(0.5)**m)\n",
    "        # avoid to stay stalled        \n",
    "        if norm(g(xk[:,k])+g(xk[:,k-1]),1)<grad_prec: \n",
    "            #print(\"gradients..\",  g(xk[k+1]),g(xk[k]))\n",
    "            mu=mu*0.99\n",
    "            xk[:,k+1]=update_grad(xk[:,k],mu)\n",
    "        if verbose: \n",
    "            if np.size(x0)==1:\n",
    "               print(\"current solution {:2.2f}, error: {:2.2e}, gradient {:2.2e}, objective {:2.2f}\".format(xk[0,k+1],err,g(xk[0,k+1]),f(xk[0,k+1])))\n",
    "            else:\n",
    "                print(\"error: {:2.2e}, gradient {:2.2e}, objective {:2.2f}\".format(err,norm(g(xk[:,k+1]),2),f(xk[:,k+1])))\n",
    "               #pass     \n",
    "        k=k+1\n",
    "    return xk[:,:k]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate the SDA in the case of an bivariate quadratic function. You may experiment by modifying the initial guess and the step-size $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):  #objective function\n",
    "    return np.sum(x**2) #\n",
    "def ff(x):\n",
    "    return np.array([f(xx) for xx in x])\n",
    "\n",
    "def g(x): #gradient\n",
    "    return 2*x #\n",
    "\n",
    "#Test # -------------------------------------------------------------------\n",
    "\n",
    "def tst(ini0, ini1, mu):\n",
    "    eps=0.001\n",
    "    xk=grad_algo(f, g, mu=mu, x0=[ini0, ini1], eps=0.001, grad_prec=0.0001, itermax=200, line_search=False, verbose=False)\n",
    "    #xk=grad_algo(f, g, mu=0.05, x0=0.5, eps=0.001, grad_prec=eps/10, itermax=200, line_search=False, verbose=True)\n",
    "\n",
    "\n",
    "    x=np.linspace(-5,5,400)\n",
    "    plt.plot(x,ff(x))\n",
    "    x=xk[0,:]\n",
    "    plt.plot(x,ff(x),'o-')\n",
    "    x=xk[1,:]\n",
    "    plt.plot(x,ff(x),'o-')\n",
    "    \n",
    "    \n",
    "x0=widgets.FloatText(value=3.5)    \n",
    "x1=widgets.FloatText(value=-4.2) \n",
    "mu=widgets.FloatSlider(min=0, max=1.4, step=0.01, value=0.85)\n",
    "#c=widgets.ContainerWidget(children=(ini0, ini1))\n",
    "_=interact(tst, ini0=x0, ini1=x1, mu=mu   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also instructive to look at what happens in the case of a non-quadratic function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sum(x**4/4 - x**3/3 - 9*x**2/2 + 9*x) #np.sum(x**2) #\n",
    "def ff(x):\n",
    "    return np.array([f(xx) for xx in x])\n",
    "\n",
    "def g(x):\n",
    "    return((x-1)*(x+3)*(x-3)) # 2*x #\n",
    "\n",
    "#Test # -------------------------------------------------------------------\n",
    "\n",
    "def tst(ini0, ini1, mu):\n",
    "    eps=0.001\n",
    "    xk=grad_algo(f, g, mu=mu, x0=[ini0, ini1], eps=0.001, grad_prec=0.0001, itermax=200, line_search=False, verbose=False)\n",
    "    #xk=grad_algo(f, g, mu=0.05, x0=0.5, eps=0.001, grad_prec=eps/10, itermax=200, line_search=False, verbose=True)\n",
    "\n",
    "\n",
    "    x=np.linspace(-5,5,400)\n",
    "    plt.plot(x,ff(x))\n",
    "    x=xk[0,:]\n",
    "    plt.plot(x,ff(x),'o-')\n",
    "    x=xk[1,:]\n",
    "    plt.plot(x,ff(x),'o-')\n",
    "    plt.figure()\n",
    "\n",
    "\n",
    "    x=np.linspace(-5,5,100)\n",
    "    xx,yy=meshgrid(x,x)\n",
    "    z=np.zeros((len(xx),len(yy)))\n",
    "    for m,a in enumerate(x):\n",
    "        for n,b in enumerate(x):\n",
    "            z[n,m]=f(array([a,b]))\n",
    "            #print(m,n,a,b,z[m,n])\n",
    "    #z=[[f(array([a,b])) for a in xx] for b in yy]\n",
    "    h = plt.contour(x,x,z,20)\n",
    "    plt.plot(xk[0,:],xk[1,:], 'o-')\n",
    "    \n",
    "x0=widgets.FloatText(value=0.5)   # or -1.5 \n",
    "x1=widgets.FloatText(value=1.2)   # 0.8\n",
    "mu=widgets.FloatSlider(min=0, max=1.4, step=0.01, value=0.07)\n",
    "#c=widgets.ContainerWidget(children=(ini0, ini1))\n",
    "_=interact(tst, ini0=x0, ini1=x1, mu=mu   )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to the iterative resolution of the normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x=np.linspace(-5,5,400)\n",
    "y=(x-1)*(x+3)*(x-3) # \n",
    "y=x**4/4 - x**3/3 - 9*x**2/2 + 9*x\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "x=sympy.symbols('x')\n",
    "e=sympy.expand((x-1)*(x+3)*(x-3))\n",
    "print(e)\n",
    "sympy.integrate(e)\n",
    "sympy.plot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions\n",
    "$$\\def\\ub{\\mathbf{u}}\n",
    "\\def\\wb{\\mathbf{w}}\n",
    "\\def\\wbopt{\\mathop{\\mathbf{w}}\\limits^\\triangle}\n",
    "\\def\\deriv#1#2{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n",
    "\\def\\Ub{\\mathbf{U}}\n",
    "\\def\\db{\\mathbf{d}}\n",
    "\\def\\eb{\\mathbf{e}}\n",
    "\\def\\vb{\\mathbf{v}}\n",
    "\\def\\Ib{\\mathbf{I}}\n",
    "\\def\\Vb{\\mathbf{V}}\n",
    "\\def\\Lambdab{\\mathbf{\\Lambda}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the MMSE criterion  \n",
    "\\begin{align}\n",
    "J_\\mathrm{mse}(w)  & =  \\E{e(n)^2}\\\\\n",
    " & = \\wb^T \\Rb_{uu} \\wb - 2\\wb^T \\Rb_{du} +\\sigma_d^2 \n",
    "\\end{align}\n",
    "has for gradient vector\n",
    "\\begin{align}\n",
    "\\nabla_{\\wb} J_\\mathrm{mse}(\\wb) & = 2 \\E{ \\ub(n) e(n) } \\\\ \n",
    "& = 2 \\left(\\Rb_{uu} \\wb -  \\Rb_{du} \\right).     \n",
    "\\end{align}\n",
    "The derivative is zero if and only if $\\Rb_{uu} \\wb = \\Rb_{du} $ which is the normal equation. \n",
    "\n",
    "Instead of directly solving the normal equation by taking the inverse of $\\Rb_{uu}$, we can also minimize the original criterion using a SDA algorithm. Since the MMSE criterion is a quadratic form in $\\wb$, it has an only minimum $\\wbopt$ which will be reached regardless of the initial condition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Beginning  with the general formulation (\\ref{eq:grad_algo}) of the SDA, and using the expression of the gradient of the MMSE, we readily obtain\n",
    "$$\n",
    "\\eqboxd{\n",
    "\\begin{align}\n",
    "\\wb(k+1) & = \\wb(k) - \\mu \\E{ \\ub(n) e(n) } \\\\\n",
    "& = \\wb(k) - \\mu \\left(\\Rb_{uu} \\wb(k) -  \\Rb_{du} \\right) \\\\\n",
    "\\end{align}\n",
    "}\n",
    "\\label{eq:grad}\n",
    "$$\n",
    "(we absorbed the factor 2 in the gradient into the constant $\\mu$). It is important to stress that here, the index $k$ represents the iterations of the algorithm, \\textem{and has nothing to do with time}. \\footnote{Actually the normal equation we are solving is independent of time -- it is only in the non-stationary case that normal equation depends on time; in such a case, the SDA would depend on both iterations and time.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even before studying convergence properties of the resulting algorithm, let us examine its behavior in the very same example of filter identification we used in section \\ref{sec:filterident}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(749)\n",
    "from scipy.signal import lfilter\n",
    "# test\n",
    "N=800\n",
    "x=lfilter([1, 1], [1], np.random.randn(N))\n",
    "htest=10*np.array([1, 0.7, 0.7, 0.7, 0.3, 0 ])\n",
    "y=lfilter(htest,[1],x)+0.1*randn(N)\n",
    "plt.plot(y)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Observation\")\n",
    "figcaption(\"System output in an identification problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that iterates the SDA, beginning with an initial condition `winit` until the (norm of the) increment between two successive updates is less than a given precision `eps` (use a `while` loop). The syntax of the function should be\n",
    "```python\n",
    "sda(Ruu,Rdu, winit, mu, eps)\n",
    "```\n",
    "It will return the optimum vector w and the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# DO IT YOURSELF!\n",
    "#def sda(Ruu,Rdu, winit, mu=0.05, eps=0.001):\n",
    "#    itermax=2000\n",
    "#    err=100\n",
    "#    k=0\n",
    "#    w=winit\n",
    "#    while ...\n",
    "#\n",
    "#    return w,niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "def sda(Ruu,Rdu, winit, mu=0.05, eps=0.001, verbose=False):\n",
    "    itermax=2000\n",
    "    err=(100, 100)\n",
    "    k=0\n",
    "    w=winit\n",
    "    while np.linalg.norm(err,2)>eps and k<itermax-1:\n",
    "        err=(Ruu.dot(w)-Rdu)\n",
    "        w=w-mu*err    \n",
    "        k+=1\n",
    "        if verbose: print(\"Iteration {0:d}, error: {1:2.2e}\".format(k,np.linalg.norm(err,2)))\n",
    "    return w, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we test the implementation with $\\mu=0.05$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from correlation import xcorr\n",
    "from scipy.linalg import toeplitz\n",
    "from numpy.linalg import inv\n",
    "mu=0.05\n",
    "(u, d, q)=(x, y, 6) \n",
    "c=xcorr(u,u,maxlags=q)[0][q::] #correlation vector\n",
    "Ruu=toeplitz(c)\n",
    "Rdu=xcorr(d,u,maxlags=q)[0][q::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w, nbiter=sda(Ruu,Rdu, winit=np.zeros(q+1), mu=0.1, eps=0.001, verbose=False)\n",
    "print(\"for mu={0:1.3f}, number of iterations: {1:}\".format(mu,nbiter))\n",
    "print(\"Identified filter\", w)\n",
    "print(\"True filter\", htest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also study the behavior and performance of the SDA as a function of the step-size $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from correlation import xcorr\n",
    "from scipy.linalg import toeplitz\n",
    "from numpy.linalg import inv\n",
    "\n",
    "u=x\n",
    "d=y\n",
    "q=6\n",
    "c=xcorr(u,u,maxlags=q)[0][q::] #correlation vector\n",
    "Ruu=toeplitz(c)\n",
    "Rdu=xcorr(d,u,maxlags=q)[0][q::]\n",
    "wopt=inv(Ruu).dot(Rdu)\n",
    "k=0\n",
    "mu_iter=np.arange(0,0.52,0.01)\n",
    "niter=np.empty(np.shape(mu_iter))\n",
    "\n",
    "for mu in mu_iter:\n",
    "    w, nbiter=sda(Ruu,Rdu, winit=np.zeros(q+1), mu=mu, eps=0.001, verbose=False)\n",
    "    niter[k]=nbiter\n",
    "    k+=1\n",
    "    #print(\"for mu={0:1.3f}, number of iterations: {1:}\".format(mu,nbiter))\n",
    "print(\"Last identified filter\", w)\n",
    "print(\"true filter\", htest)\n",
    "plt.plot(mu_iter,niter)\n",
    "plt.xlabel(\"$\\mu$\")\n",
    "plt.ylabel(\"Number of iterations\")\n",
    "figcaption(\"Number of iterations of the gradient algorithm as a function of $\\mu$\", \n",
    "           label=\"fig:itergrad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_iter[np.argmin(niter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the number of iterations needed to obtain the convergence (up to a given precision) essentially decreases with $\\mu$, up to a minimum. After this minimum, the number of iterations shortly increases, up to a value of $\\mu$ where the algorithm begins to diverge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the step-size is crucial. If the steps are too large, then the algorithm may diverge. If they are too small, then convergence may take a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditions on the step-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\vb(n)=\\wb(n)-\\wbopt$ denote the error between the filter at step $n$ and the optimum filter $\\wbopt$. Subtracting $\\wbopt$ from both sides of the SDA\n",
    "$$\n",
    "\\wb(n+1) = \\wb(n) - \\mu \\left(\\Rb_{uu} \\wb(n) -  \\Rb_{du} \\right)\n",
    "$$\n",
    "we get\n",
    "$$\n",
    "\\vb(n+1) = \\vb(n) - \\mu \\left(\\Rb_{uu} \\wb(n) -  \\Rb_{du} \\right).\n",
    "$$\n",
    "Using the fact that $\\Rb_{du}=\\Rb_{uu} \\wbopt(n)$, it comes\n",
    "\\begin{align}\n",
    "\\vb(n+1) & = \\vb(n) - \\mu \\left(\\Rb_{uu} \\wb(n) -  \\Rb_{uu} \\wbopt(n) \\right) \\\\ \n",
    " & = \\vb(n) - \\mu  \\Rb_{uu} \\vb(n) \\\\\n",
    "& = \\left(\\Ib-\\mu\\Rb_{uu}\\right)\\vb(n).\n",
    "\\end{align}\n",
    "\n",
    "It is then immediate to express the error at iteration $n+1$ in terms of the initial error $\\vb(0)$:\n",
    "$$\n",
    "\\vb(n+1) =   \\left(\\Ib-\\mu\\Rb_{uu}\\right)^{n+1}\\vb(0).\n",
    "$$\n",
    "Clearly, if the algorithm converges, the error shall tends to zero and so doing forget the initial conditions. Here, the error decreases to zero if $\\left(\\Ib-\\Rb_{uu}\\right)^{n+1}$ tends to the null matrix. This happens if all the eigenvalues of $\\left(\\Ib-\\Rb_{uu}\\right)$ have a modulus inferior to 1. To see this, let us introduce the eigen-decomposition of $\\Rb_{uu}$:\n",
    "$$\n",
    "\\Rb_{uu} = \\Vb \\Lambdab\\Vb^H\n",
    "$$\n",
    "where $\\Vb$ is the matrix of right eigenvectors of $\\Rb_{uu}$, and $\\Lambdab$ the corresponding diagonal matrix of eigenvalues. The superscript $^H$ indicates the conjugate transposition (that is transposition plus conjugation). In the case of a correlation matrix, the eigenvalues are all non-negative, and the eigenvectors can be chosen normed and orthogonal to each other. In other terms, $\\Vb$ is unitary:\n",
    "$$\n",
    "\\Vb \\Vb^H = \\Ib \\text{ or } \\Vb^{-1}= \\Vb^H. \n",
    "$$\n",
    "Therefore, $\\left(\\Ib-\\mu\\Rb_{uu}\\right)$ can be put under the form $\\Vb\\left(\\Ib-\\mu\\Lambdab\\right)\\Vb^H$. This shows that the eigenvalues of the matrix have the form $1-\\mu \\lambda_i$, where the $\\lambda_i$ are the eigenvalues of the correlation matrix. For the power $(n+1)$ we then obtain\n",
    "$$\n",
    "\\left(\\Ib-\\mu\\Rb_{uu}\\right)^{n+1} = \\Vb\\left(\\Ib-\\mu\\Lambdab\\right)^{n+1}\\Vb^H.\n",
    "\\label{eq:matpowern}\n",
    "$$\n",
    "Hence we see that this matrix will converge to zero if and only if \n",
    "$$\n",
    "|1-\\mu \\lambda_i|< 1 {~~~~}\\forall i.\n",
    "$$\n",
    "- If $1-\\mu \\lambda_i>0$, this yields $1-\\mu \\lambda_i< 1$ and therefore since $\\lambda_i\\geq 0$ implies $\\mu\\geq0$;\n",
    "- If $1-\\mu \\lambda_i<0$, we obtain  $\\mu \\lambda_i-1< 1$, so that $\\mu<2/\\lambda_i$. Since this must be true for all $\\lambda_i$, we can only keep the most restrictive inequality: $\\mu<2/\\lambda_\\mathrm{max}$, where $\\lambda_\\mathrm{max}$ denotes the maximum eigenvalue. \n",
    "\n",
    "Finally, we obtain the following condition\n",
    "$$\n",
    "\\eqboxd{\n",
    "\\displaystyle{\n",
    "0 \\leq \\mu<\\frac{2}{\\lambda_\\mathrm{max}}\n",
    "}\n",
    "}\n",
    "$$\n",
    "on the step-size, for ensuring the convergence of the algorithm. \n",
    "\n",
    "\\footnote{\n",
    "The correlation matrix $\\Rb_{uu}$ is non-negative definite. This means that $\\forall \\vb$, $\\vb^H\\Rb_{uu}\\vb\\geq0$. This is easy to check. Indeed, since $\\Rb_{uu}=\\E{\\ub(n)\\ub(n)^H}$, we have $\\vb^H\\Rb_{uu}\\vb = \\E{\\vb^H\\ub(n)\\ub(n)^H\\vb} = \\E{||\\vb^H\\ub(n)||^2} \\geq0$. Let now $\\vb$ be any eigenvector of $\\Rb_{uu}$ with eigenvalue $\\lambda$. In such a case, we have $\\vb^H\\Rb_{uu}\\vb = \\vb^H\\lambda\\vb=\\lambda||\\vb||^2$. Since we just seen that $\\vb^H\\Rb_{uu}\\vb \\geq 0$, we deduce that all the eigenvalues $\\lambda$ are non-negative. \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimum step-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From (\\ref{eq:matpowern}), we see that the speed of convergence will be limited by slowest eigenvalue, that is by the eigenvalue whose modulus is the nearest to one. Hence, in order to minimize the convergence time, we have to select the maximum of the $1-\\mu\\lambda_k$, with respect to $k$, and minimize that value with respect to $\\mu$. Hence we face a [minimax](http://en.wikipedia.org/wiki/Minimax) problem: \n",
    "$$\n",
    "\\min_\\mu \\max_k |1-\\mu\\lambda_k|\n",
    "\\label{eq:minmax}\n",
    "$$\n",
    "Suppose that there exists a $\\mu_\\mathrm{opt}$ that realizes the minimum with respect to $\\mu$. For $\\mu>\\mu_\\mathrm{opt}$, we then have\n",
    "$$\n",
    "\\mu\\lambda_k -1 > \\mu_\\mathrm{opt} \\lambda_k-1 \\geq \\mu_\\mathrm{opt} \\lambda_\\mathrm{min}-1 \n",
    "$$\n",
    "On the other hand, for $\\mu<\\mu_\\mathrm{opt}$, we  have\n",
    "$$\n",
    "\\mu\\lambda_k -1 < \\mu_\\mathrm{opt} \\lambda_k-1\n",
    "$$\n",
    "or\n",
    "$$\n",
    "1-\\mu\\lambda_k > 1-\\mu_\\mathrm{opt} \\lambda_k \\geq 1-\\mu_\\mathrm{opt} \\lambda_\\mathrm{max}\n",
    "$$\n",
    "Hence, we obtain that the solution of (\\ref{eq:minmax}) is $1-\\mu_\\mathrm{opt} \\lambda_\\mathrm{max}$ from above and $\\mu_\\mathrm{opt} \\lambda_\\mathrm{min}-1$ from below. Of course we have, by continuity, \n",
    "$$\n",
    "1-\\mu_\\mathrm{opt} \\lambda_\\mathrm{max} = \\mu_\\mathrm{opt} \\lambda_\\mathrm{min}-1\n",
    "$$\n",
    "which yields \n",
    "$$\n",
    "\\eqboxc{\n",
    "\\displaystyle{\n",
    "\\mu_\\mathrm{opt} = \\frac{2}{ \\lambda_\\mathrm{max} +  \\lambda_\\mathrm{min} }\n",
    "}}\n",
    "\\label{eq:muopt}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two results: convergence condition and optimum step-size completely corresponds to what we observed numerically in figure \\ref{fig:itergrad}. If we compute the eigenvalues of the correlation matrix $\\Rb_{uu}$, we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L,V=np.linalg.eig(Ruu)\n",
    "print(\"Maximum step-size: \", 2/(np.max(L)))\n",
    "print(\"Optimum step-size: \", 2/(np.max(L)+np.min(L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth recalling that we introduced the iterative algorithm in order to avoid the direct inversion of the correlation matrix, possibly for computational load reasons. However, computing the eigenvalues of the correlation matrix is at least as complicated as computing the inverse. Thus we do not gain anything if we compute the optimum step-size (\\ref{eq:muopt}). Fortunately, we can use the following value:\n",
    "$$\n",
    "\\mu = \\frac{2}{\\mathrm{Tr}\\left[\\Rb_{uu}\\right]},\n",
    "$$\n",
    "where $\\mathrm{Tr}$ denotes the trace operator, that is the sum of the eigenvalues. Sine we know that the trace is also the sum of therms in the main diagonal, and since the matrix is Toeplitz, we also have\n",
    "$$\n",
    "\\mu = \\frac{2}{pR_{uu}(0)},\n",
    "$$\n",
    "where $R_{uu}(0)=\\E{|u(n)|^2}$ and $p$ is the dimension of the correlation matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative view of the Steepest Descent Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The sum of $n$ terms of a geometric series of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{proposition}\n",
    "The sum of the first $n$ terms of the geometric series $B^k$, where $B$ is any square matrix\n",
    "$$\n",
    "S_n=B^0+B+B^2+\\ldots+B^k+\\ldots+B^n  \n",
    "\\label{eq:geometricseries}\n",
    "$$\n",
    "is given by \n",
    "\\begin{equation}\n",
    "\\eqboxd{\n",
    "S_n=(1-B)^{-1}(1-B^{n+1})\n",
    "}.\n",
    "\\label{eq:sumgeometricseries}\n",
    "\\end{equation}\n",
    "\n",
    "If the [spectral radius](http://en.wikipedia.org/wiki/Spectral_radius) of $B$ is less than 1, then $\\lim_{n\\rightarrow\\infty} B^n=0$, and \n",
    "$$\n",
    "\\eqboxc{\n",
    "S_\\infty=(1-B)^{-1}\n",
    "}.\n",
    "\\label{eq:limsumgeometricseries}\n",
    "$$\n",
    "\\end{proposition}\n",
    "\n",
    "\\begin{proof}\n",
    "Consider the geometric series\n",
    "$$\n",
    "B^0+B+B^2+\\ldots+B^k+\\ldots\n",
    "$$\n",
    "where B is any matrix. The sum of the first $n$ terms of this geometric series is given by (\\ref{eq:geometricseries}).\n",
    "Of course, we also have \n",
    "\\begin{align}\n",
    "BS_n & =B+B^2+\\ldots+B^n+B^{n+1} \\\\\n",
    "   & = S-1+ B^{n+1} . \n",
    "\\end{align}   \n",
    "Therefore we have\n",
    "$$\n",
    "(B-1)S_n=-1+ B^{n+1},\n",
    "$$\n",
    "and finally the result (\\ref{eq:sumgeometricseries}) follows after applying the left inverse of $(B-1)$ to both sides.\n",
    "\\end{proof}\n",
    "\n",
    "**Application** -- This can be applied for instance to the matrix $B=1-\\mu A$. Here it gives\n",
    "$$\n",
    "\\eqboxb{\n",
    "\\displaystyle{\n",
    "\\mu S_\\infty=\\mu \\sum_{k=0}^{+\\infty} (1-\\mu A)^k = A^{-1}\n",
    "}}\\label{eq:inv_by_series}\n",
    "$$\n",
    "provided that the spectral radius of $(1-\\mu A)$ is less than one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{remark} If $B$ has a spectral radius less than one, then $(1-B)$ is invertible. \n",
    "Consider the eigendecomposition of $B$ as:\n",
    "$$\n",
    "B=V\\Lambda V^{-1},\n",
    "$$\n",
    "where V is the matrix of right eigenvectors of $B$, and $\\Lambda$ the corresponding diagonal matrix of eigenvalues. Then $(1-A)=(VV^{-1}- V \\Lambda V^{-1})=V(1-\\Lambda)V^{-1}$. The last relation is noting but a possible eigendecomposition of $(1- B)$. This shows that the corresponding eigenvalues have the form $1-\\lambda_i$. If all the eigenvalues have a modulus inferior to 1, then $1-\\lambda_i$ is never equal to zero and the matrix $(1-B)$ is invertible. \n",
    "\\end{remark}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate numerically that the sum of the geometric series generated by $B$ is indeed $(I-B)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We generate a random matrix B, compute its eigendecomposition and normalize by the maximum\n",
    "# eignevalue. Therefore, the spectal radius is inferior to 1, and the property applies\n",
    "p=50\n",
    "B=np.random.randn(p,p)\n",
    "L,V= np.linalg.eig(B)\n",
    "ll=np.max(np.abs(L))\n",
    "B=B/(1.1*ll)\n",
    "\n",
    "# Now we compute the true inverse of (I-B):\n",
    "I=np.eye(p)\n",
    "IBi=np.linalg.inv(I-B)\n",
    "\n",
    "N=50  # number of terms in the sum\n",
    "err=np.zeros(N) # Evolution of error\n",
    "S=np.zeros(p)\n",
    "C=I   # initial C\n",
    "\n",
    "for k in np.arange(N):\n",
    "    S=S+C\n",
    "    C=C.dot(B)\n",
    "    err[k]=np.linalg.norm(IBi-S,2)\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.plot((abs(err)), label=\"$||\\sum_{k=0}^N B^k - (I-B)^{-1}||^2$\")\n",
    "plt.title(\"Evolution of  the error\")\n",
    "plt.xlabel(\"k\")\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An iterative formula for computing the solution of the normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now return to the normal equation \n",
    "$$\n",
    "\\wb = \\Rb_{uu}^{-1} \\Rb_{du}. \n",
    "$$\n",
    "By the property (\\ref{eq:inv_by_series}), the inverse of the correlation matrix can be computed as\n",
    "$$\n",
    "\\Rb_{uu}^{-1} = \\mu \\sum_{k=0}^{+\\infty} (1-\\mu \\Rb_{uu})^k.\n",
    "$$\n",
    "Therefore, if we use a sum of order $n$, we have\n",
    "$$\n",
    "\\wb(n) = \\mu \\sum_{k=0}^{n} (1-\\mu \\Rb_{uu})^k  \\Rb_{du}. \n",
    "$$\n",
    "The term of rank $(n+1)$ can then be expressed as\n",
    "\\begin{align}\n",
    "\\wb(n+1) & = \\mu \\sum_{k=0}^{n+1} (1-\\mu \\Rb_{uu})^k  \\Rb_{du} \\\\\n",
    "& = \\mu (1-\\mu \\Rb_{uu}) \\sum_{k=0}^{n} (1-\\mu \\Rb_{uu})^k  \\Rb_{du} + \\mu (1-\\mu \\Rb_{uu})^0  \\Rb_{du}\\\\\n",
    "& = (1-\\mu \\Rb_{uu}) \\wb(n) + \\mu   \\Rb_{du}\\\\\n",
    "\\end{align}\n",
    "that is also\n",
    "$$\n",
    "\\eqboxc{\n",
    "\\wb(n+1)  = \\wb(n) -\\mu (\\Rb_{uu} \\wb(n) -  \\Rb_{du})\n",
    "}\n",
    "$$\n",
    "Hence, we obtain an iterative formula for computing the solution of the normal equation (\\ref{eq:normaleq}), without explicitly computing the inverse of the correlation matrix. It is an exact algorithm, which converges to the true solution:\n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty} \\wb(n) = \\wbopt =  \\Rb_{uu}^{-1} \\Rb_{du}.\n",
    "$$\n",
    "As we saw above, this algorithm also appears as a steepest descent algorithm applied to the minimization of the Mean Square Error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few references --\n",
    "\n",
    "[http://nowak.ece.wisc.edu/ece830/ece830_spring13_adaptive_filtering.pdf](http://nowak.ece.wisc.edu/ece830/ece830_spring13_adaptive_filtering.pdf)\n",
    "\n",
    "[http://www.roma1.infn.it/exp/cuore/pdfnew/ch07.pdf](http://www.roma1.infn.it/exp/cuore/pdfnew/ch07.pdf)\n",
    "\n",
    "[http://www.ece.utah.edu/~mathews/ece6550/chapter4.pdf](http://www.ece.utah.edu/~mathews/ece6550/chapter4.pdf)\n",
    "\n",
    "[http://en.wikipedia.org/wiki/Least_mean_squares_filter#Normalised_least_mean_squares_filter_.28NLMS.29](http://en.wikipedia.org/wiki/Least_mean_squares_filter#Normalised_least_mean_squares_filter_.28NLMS.29)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<div align=right> [Index](toc.ipynb) - [Back](Optimum_filtering.ipynb) - [Next](Adaptive_versions.ipynb)</div>"
   ]
  }
 ],
 "metadata": {
  "interactive_sols": {
   "cbx_id": 2
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc-wrapper_display": "none",
   "toc_cell": true,
   "toc_number_sections": false,
   "toc_section_display": "none",
   "toc_threshold": 4,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "516.177px",
   "right": "20px",
   "top": "27.8125px",
   "width": "286px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
